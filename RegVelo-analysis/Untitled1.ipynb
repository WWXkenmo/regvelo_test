{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69672b11-7be0-4b4c-ba2d-c880958810e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weixu.wang/miniconda3/envs/RegVelo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/weixu.wang/miniconda3/envs/RegVelo/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'dlopen(/Users/weixu.wang/miniconda3/envs/RegVelo/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Symbol not found: (__ZN3c106detail19maybe_wrap_dim_slowIxEET_S2_S2_b)\n",
      "  Referenced from: '/Users/weixu.wang/miniconda3/envs/RegVelo/lib/python3.10/site-packages/torchvision/image.so'\n",
      "  Expected in: '/Users/weixu.wang/miniconda3/envs/RegVelo/lib/python3.10/site-packages/torch/lib/libc10.dylib''If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing_extensions import Literal\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from scvi.nn import Encoder, FCLayers\n",
    "import scvelo as scv\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import sctour as rgv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd259818-1325-4225-8d3c-03b90d9c29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scv.set_figure_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce27c122-bfd1-4844-beff-82f6b2a62d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\"dataset_branch_v2.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92db3bd-521c-4428-9cbf-bfb74f2ff79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 1000 × 280\n",
       "    obs: 'step_ix', 'simulation_i', 'sim_time'\n",
       "    var: 'module_id', 'basal', 'burn', 'independence', 'color', 'is_tf', 'is_hk', 'transcription_rate', 'splicing_rate', 'translation_rate', 'mrna_halflife', 'protein_halflife', 'mrna_decay_rate', 'protein_decay_rate', 'max_premrna', 'max_mrna', 'max_protein', 'mol_premrna', 'mol_mrna', 'mol_protein'\n",
       "    uns: 'network', 'regulators', 'skeleton', 'targets', 'traj_dimred_segments', 'traj_milestone_network', 'traj_progressions'\n",
       "    obsm: 'dimred'\n",
       "    layers: 'counts_protein', 'counts_spliced', 'counts_unspliced', 'logcounts', 'rna_velocity'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174b7422-8477-4d90-a8ec-0d9b90aae5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(adata):\n",
    "    reg_index = [i in adata.var.index.values for i in adata.uns[\"regulators\"]]\n",
    "    tar_index = [i in adata.var.index.values for i in adata.uns[\"targets\"]]\n",
    "    adata.uns[\"regulators\"] = adata.uns[\"regulators\"][reg_index]\n",
    "    adata.uns[\"targets\"] = adata.uns[\"targets\"][tar_index]\n",
    "    W = adata.uns[\"skeleton\"]\n",
    "    W = W[reg_index,:]\n",
    "    W = W[:,tar_index]\n",
    "    adata.uns[\"skeleton\"] = W\n",
    "    W = adata.uns[\"network\"]\n",
    "    W = W[reg_index,:]\n",
    "    W = W[:,tar_index]\n",
    "    adata.uns[\"network\"] = W\n",
    "    \n",
    "    regulators = adata.uns[\"regulators\"][adata.uns[\"skeleton\"].sum(1) > 0]\n",
    "    targets = adata.uns[\"targets\"][adata.uns[\"skeleton\"].sum(0) > 0]\n",
    "\n",
    "    W = pd.DataFrame(adata.uns[\"skeleton\"],index = adata.uns[\"regulators\"],columns = adata.uns[\"targets\"])\n",
    "    W = W.loc[regulators,targets]\n",
    "    adata.uns[\"skeleton\"] = W\n",
    "    W = pd.DataFrame(adata.uns[\"network\"],index = adata.uns[\"regulators\"],columns = adata.uns[\"targets\"])\n",
    "    W = W.loc[regulators,targets]\n",
    "    adata.uns[\"network\"] = W\n",
    "    \n",
    "    adata.uns[\"regulators\"] = regulators\n",
    "    adata.uns[\"targets\"] = targets\n",
    "    \n",
    "    adata = adata[:,np.unique(adata.uns[\"regulators\"].tolist()+adata.uns[\"targets\"].tolist())].copy()\n",
    "    \n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0c149f-68f0-46cf-ad26-500bb49952b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280, 280)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.uns[\"skeleton\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7332959-3079-4cb9-a1b5-94a5ac77a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X = adata.X.copy()\n",
    "adata.layers[\"spliced\"] = adata.layers[\"counts_spliced\"].copy()\n",
    "adata.layers[\"unspliced\"] = adata.layers[\"counts_unspliced\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbe2f51b-4ba5-470c-aa74-b8fbff0a3239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 14 genes that are detected 5 counts (shared).\n",
      "Normalized count data: X, spliced, unspliced.\n",
      "Skip filtering by dispersion since number of variables are less than `n_top_genes`.\n",
      "WARNING: Did not modify X as it looks preprocessed already.\n",
      "computing neighbors\n",
      "    finished (0:00:02) --> added \n",
      "    'distances' and 'connectivities', weighted adjacency matrices (adata.obsp)\n",
      "computing moments based on connectivities\n",
      "    finished (0:00:00) --> added \n",
      "    'Ms' and 'Mu', moments of un/spliced abundances (adata.layers)\n"
     ]
    }
   ],
   "source": [
    "scv.pp.filter_and_normalize(adata, min_shared_counts=5, n_top_genes=280)\n",
    "scv.pp.moments(adata, n_pcs=30, n_neighbors=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88e05e73-465f-4545-9695-329b9e3db239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1086.6066"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.layers[\"Mu\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c9bc0da-c912-4276-bf8b-d0d2a2aeb74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X = np.log1p(adata.X.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d966d10e-c87f-4409-958c-a704b6e75084",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sanity_check(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49585f82-ba8a-4eba-b56c-4743252d10c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135, 250)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.uns[\"skeleton\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "818a857a-e951-4871-b5b8-434446a1c09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39747.324"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.layers[\"Ms\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9d2ad1d-e8bd-4587-b3ac-9882336966ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = adata.uns[\"skeleton\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a48a9e70-b213-4b99-b7c6-a0dcb6c1997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "W = torch.tensor(np.array(W)).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2b39000-bad3-41c1-99cd-5e3c4c0ab342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([135, 250])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca6fde63-4dd2-42ab-9d08-4a81b73a7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgv_m = rgv.train.Trainer(adata, W=W.T,early_stopping = False, nepoch = 200, solver = \"AdaBelief\", lr = 0.01,wt_decay=0.01,T_max=300,batch_size=128,grad_clip = 5,alpha_recon_reg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "1eb0c6df-5526-44ba-888f-e203abdaef0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n",
      "\u001b[31mModifications to default arguments:\n",
      "\u001b[31m                           eps  weight_decouple    rectify\n",
      "-----------------------  -----  -----------------  ---------\n",
      "adabelief-pytorch=0.0.5  1e-08  False              False\n",
      ">=0.1.0 (Current 0.2.0)  1e-16  True               True\n",
      "\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n",
      "----------------------------------------------------------  ----------------------------------------------\n",
      "Recommended eps = 1e-8                                      Recommended eps = 1e-16\n",
      "\u001b[34mFor a complete table of recommended hyperparameters, see\n",
      "\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n",
      "\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n",
      "\u001b[0m\n",
      "Weight decoupling enabled in AdaBelief\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200: 100%|████████████████████████████████| 200/200 [12:19<00:00,  3.70s/epoch, train_loss=1.83e+7, val_loss=1.62e+7]\n"
     ]
    }
   ],
   "source": [
    "rgv_m.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d672d96-acbd-4afb-99a6-afc48d5cd149",
   "metadata": {},
   "outputs": [],
   "source": [
    "class alpha_encoder(nn.Module):\n",
    "    \"\"\" \n",
    "    encode the time dependent alpha (f)\n",
    "    time dependent transcription rate is determined by upstream emulator\n",
    "\n",
    "    \"\"\"                 \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_int: int = 5,\n",
    "        alpha_unconstr_init: torch.Tensor = None,\n",
    "        W: torch.Tensor = (torch.FloatTensor(5, 5).uniform_() > 0.5).int(),\n",
    "        W_int: torch.Tensor = None,\n",
    "        log_h_int: torch.Tensor = None,\n",
    "        global_time: bool = False,\n",
    "    ):\n",
    "        device = W.device\n",
    "        super().__init__()\n",
    "        self.n_int = n_int\n",
    "        if global_time:\n",
    "            self.log_h = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.log_phi = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.tau = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.o = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "        else:\n",
    "            self.log_h = torch.nn.Parameter(log_h_int.repeat(W.shape[0],1)*W)\n",
    "            self.log_phi = torch.nn.Parameter(torch.ones(W.shape).to(device)*W)\n",
    "            self.tau = torch.nn.Parameter(torch.ones(W.shape).to(device)*W*10)\n",
    "            self.o = torch.nn.Parameter(torch.ones(W.shape).to(device)*W)\n",
    "\n",
    "        self.mask_m = W\n",
    "        self.global_time = global_time\n",
    "\n",
    "        ## initialize grn\n",
    "        self.grn = torch.nn.Parameter(W_int*self.mask_m)\n",
    "        \n",
    "        ## initilize gamma and beta\n",
    "        self.beta_mean_unconstr = torch.nn.Parameter(0.5 * torch.ones(n_int))\n",
    "        self.gamma_mean_unconstr = torch.nn.Parameter(-1 * torch.ones(n_int))\n",
    "        self.alpha_unconstr_bias = torch.nn.Parameter(torch.zeros(n_int))\n",
    "        self.alpha_unconstr_max = torch.nn.Parameter(torch.randn(n_int))\n",
    "        # calculating emulating matrix\n",
    "    \n",
    "    ### define hook to froze the parameters\n",
    "    def _set_mask_grad(self):\n",
    "        self.hooks_grn = []\n",
    "        if not self.global_time:\n",
    "            self.hooks_log_h = []\n",
    "            self.hooks_log_phi = []\n",
    "            self.hooks_tau = []\n",
    "            self.hooks_o = []\n",
    "        #mask_m = self.mask_m\n",
    "        \n",
    "        def _hook_mask_no_regulator(grad):\n",
    "            return grad * self.mask_m\n",
    "\n",
    "        w_grn = self.grn.register_hook(_hook_mask_no_regulator)\n",
    "        self.hooks_grn.append(w_grn)\n",
    "        if not self.global_time:\n",
    "            w_log_h = self.log_h.register_hook(_hook_mask_no_regulator)\n",
    "            w_log_phi = self.log_phi.register_hook(_hook_mask_no_regulator)\n",
    "            w_tau = self.tau.register_hook(_hook_mask_no_regulator)\n",
    "            w_o = self.o.register_hook(_hook_mask_no_regulator)\n",
    "\n",
    "            self.hooks_log_h.append(w_log_h)\n",
    "            self.hooks_log_phi.append(w_log_phi)\n",
    "            self.hooks_tau.append(w_tau)\n",
    "            self.hooks_o.append(w_o)\n",
    "\n",
    "    def emulator(t,log_h_v,log_phi_v,tau_v,o_v):\n",
    "        pre = torch.exp(log_h_v)*torch.exp(-torch.exp(phi_v)*(t-tau_v)**2)+o_v\n",
    "        \n",
    "        return pre\n",
    "\n",
    "    def emulation_all(self,t: torch.Tensor = None):\n",
    "        if self.global_time:\n",
    "            # broadcasting the time t\n",
    "            t = t.repeat((self.mask_m.shape[0],1))\n",
    "\n",
    "        emulate_m = torch.zeros([self.mask_m.shape[0], self.mask_m.shape[1], t.shape[1]])\n",
    "\n",
    "        h = torch.exp(self.log_h)\n",
    "        phi = torch.exp(self.log_phi)\n",
    "        for i in range(t.shape[1]):\n",
    "            # for each time stamps, predict the emulator predict value\n",
    "            tt = t[:,i]\n",
    "            emu = h * torch.exp(-phi*(tt.reshape((len(tt),1))-self.tau)**2) + self.o\n",
    "            emulate_m[:,:,i] = emu\n",
    "\n",
    "        return emulate_m\n",
    "\n",
    "\n",
    "    def forward(self,t,g):\n",
    "        ## Encode \n",
    "\n",
    "        if self.global_time:\n",
    "            u = u[locate]\n",
    "            s = s[locate]\n",
    "            ## when use global time, t is a single value\n",
    "            T = t.repeat((dim,1))\n",
    "\n",
    "            ## calculate emulator vector\n",
    "            h = torch.exp(self.log_h)\n",
    "            phi = torch.exp(self.log_phi)\n",
    "            emu = h[locate,:] * torch.exp(-phi[locate,:]*(T.reshape((dim,1))-self.tau[locate,:])**2) + self.o[locate,:]\n",
    "            \n",
    "            ## Use the Emulator matrix to predict alpha\n",
    "            emu = emu * self.grn[locate,:]\n",
    "            alpha_unconstr = emu.sum(dim=1)\n",
    "            alpha_unconstr = alpha_unconstr + self.alpha_unconstr[locate]\n",
    "            \n",
    "            ## Generate kinetic rate\n",
    "            beta = torch.clamp(F.softplus(self.beta_mean_unconstr[locate]), 0, 50)\n",
    "            gamma = torch.clamp(F.softplus(self.gamma_mean_unconstr[locate]), 0, 50)\n",
    "            alpha = torch.clamp(F.softplus(alpha_unconstr),0,50)\n",
    "\n",
    "            ## Predict velocity\n",
    "            du = alpha - beta*u\n",
    "            ds = beta*u - gamma*s\n",
    "\n",
    "            du = du.reshape((dim,1))\n",
    "            ds = ds.reshape((dim,1))\n",
    "\n",
    "            v = torch.concatenate([du,ds],axis = 1)\n",
    "\n",
    "        else:\n",
    "            ## calculate emulator value\n",
    "            ## output the f_g(t)\n",
    "            \n",
    "            ## Build Emulator matrix for gene g\n",
    "            \n",
    "            h = torch.exp(self.log_h)[g,:].view(-1)\n",
    "            phi = torch.exp(self.log_phi)[g,:].view(-1)\n",
    "            tau = self.tau[g,:].view(-1)\n",
    "            o = self.o[g,:].view(-1)\n",
    "            w = self.grn[g,:].view(-1)\n",
    "            bias = self.alpha_unconstr_bias[g]\n",
    "\n",
    "            #emu = h[locate,:] * torch.exp(-phi[locate,:]*(T.reshape((dim,1))-self.tau[locate,:])**2) + self.o[locate,:]\n",
    "            emu = h * torch.exp(-phi*(t - tau)**2) + o\n",
    "\n",
    "            ## Use the Emulator matrix to predict alpha\n",
    "            #emu = emu * self.grn[locate,:]\n",
    "            emu = emu * w\n",
    "            \n",
    "            alpha_unconstr = emu.sum()\n",
    "            #alpha_unconstr = alpha_unconstr + self.alpha_unconstr_bias[locate]\n",
    "            alpha_unconstr = alpha_unconstr + bias\n",
    "\n",
    "            ## Generate transcription kinetic rate for time t\n",
    "            alpha = torch.clamp(alpha_unconstr,0,)\n",
    "            alpha = F.softsign(alpha)\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "def SolveInitialValueProblem(f_t, x0, t0, t_eval):\n",
    "    \n",
    "    ## generate the prediction of unspliced/spliced readout at time t for every gene\n",
    "    ## use torchquad integral, different with torchode, the t_eval no longer need to be ordered\n",
    "    \n",
    "    ## get the kinetic parameters\n",
    "    beta = torch.clamp(F.softplus(f_t.beta_mean_unconstr), 0, 50)\n",
    "    gamma = torch.clamp(F.softplus(f_t.gamma_mean_unconstr), 0, 50)\n",
    "    alpha_max = torch.clamp(F.softplus(f_t.alpha_unconstr_max),0,50)\n",
    "\n",
    "    ## define integral function\n",
    "    def integral_alpha_beta(f_t, tt, t0, beta, g):\n",
    "        f_i = lambda t: f_t(t,g)*torch.exp(beta[g]*t)\n",
    "        integration_domain = [[t0[g],tt]]\n",
    "        result = simp.integrate(f_i, dim=1, N=101, integration_domain=integration_domain)\n",
    "        return result\n",
    "        \n",
    "    def integral_alpha_gamma(f_t, tt,t0, gamma, g):\n",
    "        f_i = lambda t: f_t(t,g)*torch.exp(gamma[g]*t)\n",
    "        integration_domain = [[t0[g],tt]]\n",
    "        result = simp.integrate(f_i, dim=1, N=101, integration_domain=integration_domain)\n",
    "        return result\n",
    "\n",
    "    ## get the initial condition (i.e. u,s = 0,0)\n",
    "    u0 = x0[:,0].view(-1)\n",
    "    s0 = x0[:,1].view(-1)\n",
    "    pre_u = torch.zeros(t_eval.shape)\n",
    "    pre_s = torch.zeros(t_eval.shape)\n",
    "    \n",
    "    ## build for loop to generate readout for each targets\n",
    "    for g, t in enumerate(t_eval):\n",
    "        u0g = u0[g]\n",
    "        s0g = s0[g]\n",
    "        \n",
    "        ## calculate integral for gene g\n",
    "        integral_tensor_alpha_beta = torch.tensor(list(map(lambda tt: integral_alpha_beta(tt=tt,f_t=f_t,t0=t0,beta = beta,g = g), t)))\n",
    "        integral_tensor_alpha_gamma = torch.tensor(list(map(lambda tt: integral_alpha_gamma(tt=tt,f_t=f_t,t0=t0,gamma = gamma,g = g), t)))\n",
    "        \n",
    "        ug = u0g*torch.exp(-beta[g]*t) + alpha_max[g]*torch.exp(-beta[g]*t)*integral_tensor_alpha_beta\n",
    "        sg = s0g*torch.exp(-gamma[g]*t) + \\\n",
    "            ( (alpha_max[g]*beta[g])/(gamma[g] - beta[g]) )*(torch.exp(-beta[g]*t)*integral_tensor_alpha_beta - torch.exp(-gamma[g]*t)*integral_tensor_alpha_gamma) + \\\n",
    "            ( (beta[g]*u0g)/(gamma[g] - beta[g]) )*(torch.exp(-beta[g]*t) - torch.exp(-gamma[g]*t))\n",
    "        \n",
    "        pre_u[g,:] = ug\n",
    "        pre_s[g,:] = sg\n",
    "        #print(g)\n",
    "    return pre_u, pre_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1983c06c-45f9-4ed6-85f8-fc3cabe7442a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SolveInitialValueProblem(t_eval,f_t, x0, t0):\n",
    "    \n",
    "    ## generate the prediction of unspliced/spliced readout at time t for every gene\n",
    "    ## use torchquad integral, different with torchode, the t_eval no longer need to be ordered\n",
    "    \n",
    "    ## get the kinetic parameters\n",
    "    g = int(t_eval[0])\n",
    "    t = t_eval[1:]\n",
    "    beta = torch.clamp(F.softplus(f_t.beta_mean_unconstr), 0, 50)\n",
    "    gamma = torch.clamp(F.softplus(f_t.gamma_mean_unconstr), 0, 50)\n",
    "    alpha_max = torch.clamp(F.softplus(f_t.alpha_unconstr_max),0,50)\n",
    "\n",
    "    ## define integral function\n",
    "    def integral_alpha_beta(f_t, tt, t0, beta, g):\n",
    "        f_i = lambda t: f_t(t,g)*torch.exp(beta[g]*t)\n",
    "        integration_domain = [[t0[g],tt]]\n",
    "        result = simp.integrate(f_i, dim=1, N=101, integration_domain=integration_domain)\n",
    "        return result\n",
    "        \n",
    "    def integral_alpha_gamma(f_t, tt,t0, gamma, g):\n",
    "        f_i = lambda t: f_t(t,g)*torch.exp(gamma[g]*t)\n",
    "        integration_domain = [[t0[g],tt]]\n",
    "        result = simp.integrate(f_i, dim=1, N=101, integration_domain=integration_domain)\n",
    "        return result\n",
    "\n",
    "    ## get the initial condition (i.e. u,s = 0,0)\n",
    "    u0 = x0[:,0].view(-1)\n",
    "    s0 = x0[:,1].view(-1)\n",
    "    pre_u = torch.zeros(t_eval.shape)\n",
    "    pre_s = torch.zeros(t_eval.shape)\n",
    "    \n",
    "    ## build for loop to generate readout for each targets\n",
    "    ## calculate integral for gene g\n",
    "    u0g = u0[g]\n",
    "    s0g = s0[g]\n",
    "    \n",
    "    integral_tensor_alpha_beta = torch.tensor(list(map(lambda tt: integral_alpha_beta(tt=tt,f_t=f_t,t0=t0,beta = beta,g = g), t)))\n",
    "    integral_tensor_alpha_gamma = torch.tensor(list(map(lambda tt: integral_alpha_gamma(tt=tt,f_t=f_t,t0=t0,gamma = gamma,g = g), t)))\n",
    "\n",
    "    ug = u0g*torch.exp(-beta[g]*t) + alpha_max[g]*torch.exp(-beta[g]*t)*integral_tensor_alpha_beta\n",
    "    sg = s0g*torch.exp(-gamma[g]*t) + \\\n",
    "        ( (alpha_max[g]*beta[g])/(gamma[g] - beta[g]) )*(torch.exp(-beta[g]*t)*integral_tensor_alpha_beta - torch.exp(-gamma[g]*t)*integral_tensor_alpha_gamma) + \\\n",
    "        ( (beta[g]*u0g)/(gamma[g] - beta[g]) )*(torch.exp(-beta[g]*t) - torch.exp(-gamma[g]*t))\n",
    "    \n",
    "    pre = torch.cat([ug,sg])\n",
    "    #print(g)\n",
    "    return pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6aec0cda-53fd-476c-a9e3-6324466cf73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchquad import set_up_backend  # Necessary to enable GPU support\n",
    "from torchquad import Trapezoid, Simpson, Boole, MonteCarlo, VEGAS # The available integrators\n",
    "from torchquad.utils.set_precision import set_precision\n",
    "import torchquad\n",
    "simp = Simpson()\n",
    "\n",
    "indices = torch.arange(t.shape[0])\n",
    "indices = indices.repeat_interleave(t.shape[1])\n",
    "t_all = torch.cat((indices.reshape(-1,1),t.reshape(t.shape[0]*t.shape[1],1)),1)\n",
    "\n",
    "## define integral function\n",
    "def integral_alpha_beta(v,f_t, t0, beta):\n",
    "    g = int(v[0])\n",
    "    tt = v[1]\n",
    "    f_i = lambda t: f_t(t,g)*torch.exp(beta[g]*t)\n",
    "    integration_domain = [[t0[g],tt]]\n",
    "    result = simp.integrate(f_i, dim=1, N=101, integration_domain=integration_domain)\n",
    "    return result\n",
    "\n",
    "def integral_alpha_gamma(v,f_t,t0, gamma):\n",
    "    g = int(v[0])\n",
    "    tt = v[1]\n",
    "    f_i = lambda t: f_t(t,g)*torch.exp(gamma[g]*t)\n",
    "    integration_domain = [[t0[g],tt]]\n",
    "    result = simp.integrate(f_i, dim=1, N=101, integration_domain=integration_domain)\n",
    "    return result\n",
    "\n",
    "def sum_all(v):\n",
    "    return int(v.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6e30175c-38b1-4b26-8f7a-0d56f664b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.arange(t.shape[0])\n",
    "t_all = torch.cat((indices.reshape(-1,1),t),1)\n",
    "partial_func = functools.partial(SolveInitialValueProblem, f_t=f_t,x0 = x0, t0 = t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9ae548e1-e616-4fa1-9824-db0d7d316632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(partial_func(t_all[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5085a03a-513b-4462-86b6-bde1196d8601",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(map(lambda t_eval: partial_func(t_eval), t_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "2866ec50-7eb2-464c-b198-fdc60aebfbba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250, 129])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a3851fbf-2364-4210-9370-2cd716816ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:23.494697\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "output_tensor = torch.randn((250,256))\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    for i, output_row in enumerate(executor.map(partial_func, t_all)):\n",
    "        output_tensor[i] = output_row\n",
    "        \n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e487f75e-89f3-4fc5-8c60-06c1484841ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3023c15b-15d5-4ba2-9cc0-eec7939b9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "partial_func = functools.partial(integral_alpha_beta, f_t=f_t, t0 = t0, beta = beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "44b514a7-90fd-4511-9cf4-3f83f6ae3173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 3, 23, 18, 21, 13, 822388)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "starttime = datetime.datetime.now()\n",
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "486e6886-93b0-4204-bc52-6e19ad1bcd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:13.365531\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "input_tensor = torch.randn((t_all.shape[0],1))\n",
    "output_tensor = torch.empty_like(input_tensor)\n",
    "partial_func = functools.partial(integral_alpha_beta, f_t=f_t, t0 = t0, beta = beta)\n",
    "with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "    for i, output_row in enumerate(executor.map(partial_func, t_all)):\n",
    "        output_tensor[i] = output_row\n",
    "        \n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "072fd061-1e82-4876-9283-5e675197f875",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:33.851643\n"
     ]
    }
   ],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "a = torch.tensor(list(map(lambda v: integral_alpha_beta(v=v,f_t = f_t,t0=t0,beta = beta), t_all)))\n",
    "b = torch.tensor(list(map(lambda v: integral_alpha_beta(v=v,f_t = f_t,t0=t0,beta = beta), t_all)))\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "f39a0ab9-0189-4197-8dea-69781feb9c30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(467.4549, grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integral_alpha_beta(t_all[0,:],f_t = f_t,t0 = t0, beta = beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "d1bf618e-aaa8-412f-9c77-668ff82fd94c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.2917, 13.5292, 11.5073,  ..., 13.7434, 14.8046, 14.8780])"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_all[0,:].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "dfe768e4-2e5a-4b80-982d-57943677d46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.0000,   6.2917],\n",
       "        [  0.0000,  13.5292],\n",
       "        [  0.0000,  11.5073],\n",
       "        ...,\n",
       "        [249.0000,  13.7434],\n",
       "        [249.0000,  14.8046],\n",
       "        [249.0000,  14.8780]])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(t.shape[0])\n",
    "x = x.repeat_interleave(t.shape[1])\n",
    "torch.cat((x.reshape(-1,1),t.reshape(t.shape[0]*t.shape[1],1)),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "58d57c37-e68e-44e0-a081-39ce2e64e2fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3027,  0.1420,  0.7881, -0.9004, -0.3324],\n",
       "        [-0.8600, -0.1008,  1.7802, -0.9399,  0.1204],\n",
       "        [ 1.2766,  0.2640, -1.9085,  0.5039, -0.1361]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d9bc7a5-93cd-4c91-a1ba-5c82abd088db",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_t = alpha_encoder(n_int = rgv_m.model.n_targets, alpha_unconstr_init = rgv_m.model.v_encoder.alpha_unconstr_bias,log_h_int = rgv_m.model.v_encoder.log_h[0,:],\n",
    "                    W = W.T, W_int = rgv_m.model.v_encoder.grn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae74f0c6-c8d1-47d5-81a6-c3240a0e71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "t = m(torch.randn(250,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9babc6a3-e262-48b8-97b1-d7d9c8d3fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68570509-f1f0-4e8f-9926-7cdf4ad181d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.zeros((250,2))\n",
    "t0 = torch.zeros((250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0f76d005-20cd-487b-8fd0-aa32860454e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "84b38368-1e78-4e73-b7a3-4a08b36e780c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.2917, 13.5292, 11.5073,  6.2158, 11.7081,  7.6167, 10.6023, 14.8237,\n",
       "         2.7619,  9.0510, 10.1462,  7.9613, 18.2336,  5.5493,  3.8115,  7.9946,\n",
       "        11.6737,  8.7470, 14.3674,  4.1293, 12.6021, 11.3531,  9.9593, 13.2177,\n",
       "        11.7522,  5.0506, 14.9015,  5.1413, 13.6931,  8.8733,  9.1672,  4.3102,\n",
       "        10.7343, 13.2622, 17.3496,  6.5893,  8.6659,  4.7248, 14.5547, 10.4318,\n",
       "         5.8495, 16.8982,  8.1507,  7.0760,  9.2485, 11.7672, 10.9677, 15.8301,\n",
       "        15.8142,  2.8028,  8.7850,  5.1704, 14.3562,  5.5317, 10.6229,  4.0864,\n",
       "         2.2072,  8.5267,  2.2765, 10.4272, 16.4707,  9.5146, 16.3261,  8.7235,\n",
       "         2.5453, 11.2223, 15.9432,  3.7466,  5.5299, 12.8501, 15.6438,  6.8605,\n",
       "         5.8711,  3.9872,  2.9348,  3.4938,  3.8725, 10.9450,  7.5372,  9.4815,\n",
       "        16.3129, 17.6023,  6.9619, 16.5306, 15.2938,  8.3910,  9.2520, 10.5136,\n",
       "        14.5773, 10.6170, 12.6587, 11.8950,  9.0369, 12.1820, 13.9930,  7.3579,\n",
       "         1.0173,  1.9698,  2.9729, 14.0693, 14.6576, 15.3988, 12.9274,  9.6093,\n",
       "        16.2546, 11.2250, 15.8959, 11.7166,  6.3516, 17.2384, 10.6594, 13.3413,\n",
       "         9.6195, 11.7779, 11.5386,  8.9521,  9.1116, 14.8540,  9.1667,  5.5727,\n",
       "         7.5185,  7.7445,  4.6590,  5.2333,  9.3740,  2.4120,  2.3574, 10.7283])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_pre = t[0,:]\n",
    "t_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "edac7d9f-7fa5-44ce-9546-8a270660eefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:29.881662\n"
     ]
    }
   ],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "a,b = SolveInitialValueProblem(f_t = f_t, x0 = x0,t0 = t0, t_eval = t)\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5ac55825-8bfb-4f15-a10d-10d9a99ad3b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4227, 0.4186, 0.4227,  ..., 0.4227, 0.4227, 0.2320],\n",
       "        [1.9704, 1.9611, 1.9658,  ..., 1.9831, 1.9704, 1.8495],\n",
       "        [0.7740, 0.7740, 0.7740,  ..., 0.7504, 0.7503, 0.7740],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.6349, 0.6529, 0.6465,  ..., 0.6463, 0.6527, 0.6512],\n",
       "        [0.4001, 0.3839, 0.3989,  ..., 0.4006, 0.3969, 0.4000]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "26348bde-eb42-48ba-9657-021762a8609d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2948, 0.9760, 1.2915,  ..., 1.2651, 1.3062, 0.0964],\n",
       "        [5.4182, 4.4900, 4.7838,  ..., 5.8561, 5.4047, 2.6192],\n",
       "        [2.3793, 2.3892, 2.3487,  ..., 1.3490, 1.3474, 2.3738],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [1.2124, 1.9342, 1.8324,  ..., 1.7767, 1.9468, 2.0018],\n",
       "        [1.2203, 0.6343, 1.1521,  ..., 1.1820, 0.9574, 1.2271]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16172043-4abf-42ab-acce-b22804298faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = torch.clamp(F.softplus(f_t.beta_mean_unconstr), 0, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b02947a0-4168-4295-acfd-b8a664f9e22f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741,\n",
       "        0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741, 0.9741],\n",
       "       grad_fn=<ClampBackward1>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0305a52a-db5d-4240-b36b-2b8537c82638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
