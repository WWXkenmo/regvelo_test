{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d00bd185-db38-4630-9c6b-0c58ee39336a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:09.340971\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as pp\n",
    "import torch\n",
    "import torchquad as tq\n",
    "simp = tq.Simpson()\n",
    "\n",
    "def parametrized_integrand(x, a, b):\n",
    "    return torch.sqrt(torch.cos(torch.sin((a + b) * x)))\n",
    "\n",
    "\n",
    "a_params = torch.arange(400)\n",
    "b_params = torch.arange(10, 200)\n",
    "\n",
    "import datetime\n",
    "starttime = datetime.datetime.now()\n",
    "integration_domain = torch.Tensor([[0, 1]])\n",
    "simp = tq.Simpson()\n",
    "result = torch.stack([torch.Tensor([simp.integrate(lambda x: parametrized_integrand(x, a, b), dim=1, N=101, integration_domain=integration_domain) for a in a_params]) for b in b_params])\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8019647-d313-4f77-aed3-2da2f849679f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.078783\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "starttime = datetime.datetime.now()\n",
    "integration_domain = torch.Tensor([[0, 1]])\n",
    "simp = tq.Simpson()\n",
    "result = torch.stack([torch.Tensor([simp.integrate(lambda x: parametrized_integrand(x, a, b), dim=1, N=101, integration_domain=integration_domain) for a in a_params]) for b in b_params])\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "fdf58af8-baa4-448e-bb7c-240f1d2aa3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.006554\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "starttime = datetime.datetime.now()\n",
    "grid = torch.Tensor([a for a in a_params])\n",
    "\n",
    "t0 = torch.tensor([1,2,3])\n",
    "t_scale = torch.randn([3])\n",
    "tau = torch.randn([3,5])\n",
    "def integrand(t):\n",
    "    y = t*t_scale+t0\n",
    "    ones = torch.ones(5)\n",
    "    if len(y.shape) == 1:\n",
    "        y = y.reshape(1,-1)\n",
    "    tau_rep = tau.repeat(y.shape[0], 1, 1)\n",
    "    tau_rep = tau_rep.view(y.shape[0], tau.shape[0], tau.shape[1])\n",
    "    diff = (y.unsqueeze(2) - tau_rep)**2\n",
    "    return diff\n",
    "\n",
    "integration_domain = [[0, 2]]\n",
    "result_vectorized = simp.integrate(integrand, dim=1, N=1001, integration_domain=integration_domain)\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)\n",
    "\n",
    "#torch.all(torch.isclose(result_vectorized, result)) # True!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "96878086-ba83-4c11-ae1a-c6645f8978d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.5252,  2.0730,  6.9124,  0.2196,  2.8289],\n",
      "        [29.0403, 16.0388, 35.1022,  0.7679, 23.0190],\n",
      "        [57.2684, 50.2401, 52.0854, 30.5624, 36.9342]])\n"
     ]
    }
   ],
   "source": [
    "print(result_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "4f7cc258-cb30-459c-b0df-9189da978931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n",
      "torch.Size([1, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "y = t_scale+t0\n",
    "ones = torch.ones(5)\n",
    "print(y.shape)\n",
    "#result = torch.einsum('i,j->ij', y, ones)\n",
    "def multi_tau(tt,tau):\n",
    "    return tau - tt.unsqueeze(0)\n",
    "if len(y.shape) == 1:\n",
    "    y = y.reshape(1,-1)\n",
    "tau_rep = tau.repeat(y.shape[0], 1, 1)\n",
    "tau_rep = tau_rep.view(y.shape[0], tau.shape[0], tau.shape[1])\n",
    "print(tau_rep.shape)\n",
    "diff = tau_rep - y.unsqueeze(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "996b5a60-ece6-4907-ac7e-406c108678a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "109c667a-ae11-42a2-adf3-1a5c20f09206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((3, 4))\n",
    "b = torch.rand((1, 4))\n",
    "\n",
    "# repeat tensor a p times along a new dimension and reshape it to have shape (p, m, n)\n",
    "a_rep = a.repeat(b.shape[0], 1, 1)\n",
    "a_rep = a_rep.view(b.shape[0], a.shape[0], a.shape[1])\n",
    "\n",
    "# subtract each row of b from a_rep\n",
    "diff = a_rep - b.unsqueeze(1)\n",
    "\n",
    "print(a_rep.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a1bf0080-45a7-4b9d-a1e5-14fd85e0a006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1405, -0.1405, -0.1405, -0.1405, -0.1405],\n",
       "        [ 1.0997,  1.0997,  1.0997,  1.0997,  1.0997],\n",
       "        [ 0.2802,  0.2802,  0.2802,  0.2802,  0.2802]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = t_scale\n",
    "ones = torch.ones(tau.shape[1])\n",
    "torch.einsum('i,j->ij', y, ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1992250e-04b7-4871-b37a-5ae18d8944a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8.2278e-04,  1.9964e+00,  8.7565e+00])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "c62d5f56-b8a3-4cef-91a3-a34288cd1032",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5285,  0.5285,  0.5285],\n",
       "        [-0.8534, -0.8534, -0.8534],\n",
       "        [ 0.8717,  0.8717,  0.8717]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.ones(len(t_scale))\n",
    "torch.einsum('i,j->ij', t_scale, ones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e54e4e73-0940-4155-9525-31ad10015bc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([25.5562])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 * (torch.exp(torch.tensor([2.0])) - 1) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "98e2ae9c-faf2-4a56-b623-1dce8bafe322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.9303, 0.8444, 0.8637, 0.8861, 0.8624, 0.8639, 0.8791, 0.8673,\n",
       "        0.8643, 0.8757, 0.8697, 0.8648, 0.8736, 0.8710, 0.8654, 0.8720, 0.8718,\n",
       "        0.8661, 0.8708, 0.8722, 0.8668, 0.8698, 0.8724, 0.8676, 0.8691, 0.8723,\n",
       "        0.8683, 0.8685, 0.8720, 0.8691, 0.8682, 0.8716, 0.8697, 0.8680, 0.8712,\n",
       "        0.8702, 0.8680, 0.8707, 0.8706, 0.8681, 0.8702, 0.8709, 0.8683, 0.8698,\n",
       "        0.8711, 0.8686, 0.8694, 0.8711, 0.8689, 0.8691, 0.8710, 0.8693, 0.8688,\n",
       "        0.8709, 0.8697, 0.8687, 0.8706, 0.8700, 0.8686, 0.8704, 0.8703, 0.8687,\n",
       "        0.8701, 0.8705, 0.8688, 0.8698, 0.8706, 0.8689, 0.8695, 0.8707, 0.8692,\n",
       "        0.8693, 0.8707, 0.8694, 0.8691, 0.8705, 0.8697, 0.8692, 0.8708, 0.8699,\n",
       "        0.8689, 0.8703, 0.8701, 0.8689, 0.8700, 0.8703, 0.8690, 0.8698, 0.8704,\n",
       "        0.8691, 0.8696, 0.8705, 0.8693, 0.8694, 0.8705, 0.8695, 0.8692, 0.8705,\n",
       "        0.8697, 0.8691, 0.8704, 0.8699, 0.8690, 0.8702, 0.8702, 0.8690, 0.8700,\n",
       "        0.8703, 0.8690, 0.8698, 0.8704, 0.8691, 0.8696, 0.8705, 0.8693, 0.8694,\n",
       "        0.8705, 0.8695, 0.8692, 0.8705, 0.8697, 0.8690, 0.8705, 0.8699, 0.8689,\n",
       "        0.8703, 0.8701, 0.8688, 0.8701, 0.8704, 0.8688, 0.8699, 0.8706, 0.8689,\n",
       "        0.8695, 0.8709, 0.8690, 0.8692, 0.8711, 0.8692, 0.8687, 0.8712, 0.8696,\n",
       "        0.8682, 0.8713, 0.8701, 0.8675, 0.8714, 0.8709, 0.8666, 0.8713, 0.8726,\n",
       "        0.8646, 0.8707, 0.8785, 0.8532, 0.8236, 0.8470, 0.8769, 0.8731, 0.8641,\n",
       "        0.8715, 0.8723, 0.8666, 0.8701, 0.8719, 0.8678, 0.8694, 0.8716, 0.8685,\n",
       "        0.8691, 0.8713, 0.8691, 0.8688, 0.8710, 0.8696, 0.8687, 0.8707, 0.8699,\n",
       "        0.8687, 0.8704, 0.8702, 0.8687, 0.8701, 0.8704, 0.8688, 0.8699, 0.8705,\n",
       "        0.8690, 0.8696, 0.8706, 0.8692, 0.8694, 0.8706, 0.8694, 0.8692, 0.8705,\n",
       "        0.8696, 0.8691, 0.8704, 0.8698, 0.8690, 0.8702, 0.8701, 0.8690, 0.8701,\n",
       "        0.8703, 0.8690, 0.8699, 0.8704, 0.8691, 0.8697, 0.8705, 0.8692, 0.8695,\n",
       "        0.8705, 0.8694, 0.8693, 0.8705, 0.8696, 0.8691, 0.8704, 0.8698, 0.8690,\n",
       "        0.8703, 0.8701, 0.8689, 0.8701, 0.8703, 0.8689, 0.8699, 0.8705, 0.8689,\n",
       "        0.8697, 0.8708, 0.8696, 0.8693, 0.8706, 0.8694, 0.8691, 0.8707, 0.8696,\n",
       "        0.8689, 0.8706, 0.8698, 0.8688, 0.8705, 0.8701, 0.8687, 0.8702, 0.8704,\n",
       "        0.8687, 0.8700, 0.8706, 0.8688, 0.8696, 0.8708, 0.8689, 0.8693, 0.8710,\n",
       "        0.8692, 0.8689, 0.8710, 0.8695, 0.8686, 0.8710, 0.8698, 0.8684, 0.8708,\n",
       "        0.8703, 0.8682, 0.8705, 0.8707, 0.8681, 0.8701, 0.8711, 0.8682, 0.8696,\n",
       "        0.8715, 0.8684, 0.8691, 0.8718, 0.8687, 0.8684, 0.8720, 0.8692, 0.8678,\n",
       "        0.8720, 0.8699, 0.8672, 0.8718, 0.8707, 0.8666, 0.8714, 0.8717, 0.8662,\n",
       "        0.8707, 0.8729, 0.8659, 0.8695, 0.8744, 0.8658, 0.8678, 0.8762, 0.8661,\n",
       "        0.8650, 0.8788, 0.8671, 0.8596, 0.8841, 0.8705, 0.8413, 0.9121, 0.9979,\n",
       "        0.9480, 0.8510, 0.8567, 0.8866, 0.8662, 0.8611, 0.8784, 0.8700, 0.8629,\n",
       "        0.8747, 0.8716, 0.8642, 0.8724, 0.8724, 0.8653, 0.8708, 0.8728, 0.8664,\n",
       "        0.8697, 0.8728, 0.8673, 0.8689, 0.8726, 0.8682, 0.8683, 0.8723, 0.8691,\n",
       "        0.8679, 0.8718, 0.8698, 0.8678, 0.8713, 0.8703, 0.8678, 0.8707, 0.8708,\n",
       "        0.8680, 0.8702, 0.8710, 0.8682, 0.8697, 0.8712, 0.8686, 0.8693, 0.8712,\n",
       "        0.8689, 0.8690, 0.8711, 0.8693, 0.8687, 0.8709, 0.8697, 0.8686, 0.8707,\n",
       "        0.8701, 0.8686, 0.8704, 0.8704, 0.8686, 0.8701, 0.8706, 0.8687, 0.8698,\n",
       "        0.8707, 0.8689, 0.8695, 0.8707, 0.8692, 0.8692, 0.8707, 0.8694, 0.8691,\n",
       "        0.8706, 0.8697, 0.8690, 0.8704, 0.8700, 0.8690, 0.8708, 0.8701, 0.8689,\n",
       "        0.8700, 0.8703, 0.8690, 0.8698])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f288eea-a50d-4bae-8457-ab44736d8dd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 400])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integrand(torch.tensor(3)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a74c37b2-1220-4b8f-bed2-8f41b34a37ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([3,5,6]).reshape(1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4b77d26d-c850-4d3d-92e6-e95e229af15d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 3])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(3).repeat(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "33f0e760-b37a-4dc8-acc5-b3629ca1573f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 9, 9])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integrand(torch.tensor(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da698b-f1ed-420c-b7e9-416fcbda8635",
   "metadata": {},
   "source": [
    "# Test new algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69672b11-7be0-4b4c-ba2d-c880958810e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "/Users/weixu.wang/miniconda3/envs/RegVelo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/weixu.wang/miniconda3/envs/RegVelo/lib/python3.10/site-packages/flax/core/frozen_dict.py:169: FutureWarning: jax.tree_util.register_keypaths is deprecated, and will be removed in a future release. Please use `register_pytree_with_keys()` instead.\n",
      "  jax.tree_util.register_keypaths(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing_extensions import Literal\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "from scvi.nn import Encoder, FCLayers\n",
    "import scvelo as scv\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import sctour as rgv\n",
    "import torchquad as tq\n",
    "from torchquad import Boole\n",
    "simp = tq.Simpson()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd259818-1325-4225-8d3c-03b90d9c29bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scv.set_figure_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce27c122-bfd1-4844-beff-82f6b2a62d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\"dataset_branch_v2.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a92db3bd-521c-4428-9cbf-bfb74f2ff79e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnnData object with n_obs × n_vars = 1000 × 280\n",
       "    obs: 'step_ix', 'simulation_i', 'sim_time'\n",
       "    var: 'module_id', 'basal', 'burn', 'independence', 'color', 'is_tf', 'is_hk', 'transcription_rate', 'splicing_rate', 'translation_rate', 'mrna_halflife', 'protein_halflife', 'mrna_decay_rate', 'protein_decay_rate', 'max_premrna', 'max_mrna', 'max_protein', 'mol_premrna', 'mol_mrna', 'mol_protein'\n",
       "    uns: 'network', 'regulators', 'skeleton', 'targets', 'traj_dimred_segments', 'traj_milestone_network', 'traj_progressions'\n",
       "    obsm: 'dimred'\n",
       "    layers: 'counts_protein', 'counts_spliced', 'counts_unspliced', 'logcounts', 'rna_velocity'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174b7422-8477-4d90-a8ec-0d9b90aae5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_check(adata):\n",
    "    reg_index = [i in adata.var.index.values for i in adata.uns[\"regulators\"]]\n",
    "    tar_index = [i in adata.var.index.values for i in adata.uns[\"targets\"]]\n",
    "    adata.uns[\"regulators\"] = adata.uns[\"regulators\"][reg_index]\n",
    "    adata.uns[\"targets\"] = adata.uns[\"targets\"][tar_index]\n",
    "    W = adata.uns[\"skeleton\"]\n",
    "    W = W[reg_index,:]\n",
    "    W = W[:,tar_index]\n",
    "    adata.uns[\"skeleton\"] = W\n",
    "    W = adata.uns[\"network\"]\n",
    "    W = W[reg_index,:]\n",
    "    W = W[:,tar_index]\n",
    "    adata.uns[\"network\"] = W\n",
    "    \n",
    "    regulators = adata.uns[\"regulators\"][adata.uns[\"skeleton\"].sum(1) > 0]\n",
    "    targets = adata.uns[\"targets\"][adata.uns[\"skeleton\"].sum(0) > 0]\n",
    "\n",
    "    W = pd.DataFrame(adata.uns[\"skeleton\"],index = adata.uns[\"regulators\"],columns = adata.uns[\"targets\"])\n",
    "    W = W.loc[regulators,targets]\n",
    "    adata.uns[\"skeleton\"] = W\n",
    "    W = pd.DataFrame(adata.uns[\"network\"],index = adata.uns[\"regulators\"],columns = adata.uns[\"targets\"])\n",
    "    W = W.loc[regulators,targets]\n",
    "    adata.uns[\"network\"] = W\n",
    "    \n",
    "    adata.uns[\"regulators\"] = regulators\n",
    "    adata.uns[\"targets\"] = targets\n",
    "    \n",
    "    adata = adata[:,np.unique(adata.uns[\"regulators\"].tolist()+adata.uns[\"targets\"].tolist())].copy()\n",
    "    \n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc0c149f-68f0-46cf-ad26-500bb49952b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280, 280)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.uns[\"skeleton\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7332959-3079-4cb9-a1b5-94a5ac77a3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X = adata.X.copy()\n",
    "adata.layers[\"spliced\"] = adata.layers[\"counts_spliced\"].copy()\n",
    "adata.layers[\"unspliced\"] = adata.layers[\"counts_unspliced\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbe2f51b-4ba5-470c-aa74-b8fbff0a3239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered out 14 genes that are detected 5 counts (shared).\n",
      "Normalized count data: X, spliced, unspliced.\n",
      "Skip filtering by dispersion since number of variables are less than `n_top_genes`.\n",
      "WARNING: Did not modify X as it looks preprocessed already.\n",
      "computing neighbors\n",
      "    finished (0:00:01) --> added \n",
      "    'distances' and 'connectivities', weighted adjacency matrices (adata.obsp)\n",
      "computing moments based on connectivities\n",
      "    finished (0:00:00) --> added \n",
      "    'Ms' and 'Mu', moments of un/spliced abundances (adata.layers)\n"
     ]
    }
   ],
   "source": [
    "scv.pp.filter_and_normalize(adata, min_shared_counts=5, n_top_genes=280)\n",
    "scv.pp.moments(adata, n_pcs=30, n_neighbors=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88e05e73-465f-4545-9695-329b9e3db239",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1086.6066"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.layers[\"Mu\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c9bc0da-c912-4276-bf8b-d0d2a2aeb74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.X = np.log1p(adata.X.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d966d10e-c87f-4409-958c-a704b6e75084",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sanity_check(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "49585f82-ba8a-4eba-b56c-4743252d10c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135, 250)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.uns[\"skeleton\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "818a857a-e951-4871-b5b8-434446a1c09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39747.324"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adata.layers[\"Ms\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9d2ad1d-e8bd-4587-b3ac-9882336966ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = adata.uns[\"skeleton\"].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a48a9e70-b213-4b99-b7c6-a0dcb6c1997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "W = torch.tensor(np.array(W)).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2b39000-bad3-41c1-99cd-5e3c4c0ab342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([135, 250])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca6fde63-4dd2-42ab-9d08-4a81b73a7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgv_m = rgv.train.Trainer(adata, W=W.T,early_stopping = False, nepoch = 200, solver = \"AdaBelief\", lr = 0.01,wt_decay=0.01,T_max=300,batch_size=128,grad_clip = 5,alpha_recon_reg = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3796e98a-e8b9-48e6-915c-e1dd81f7a511",
   "metadata": {},
   "outputs": [],
   "source": [
    "class alpha_encoder(nn.Module):\n",
    "    \"\"\" \n",
    "    encode the time dependent alpha (f)\n",
    "    time dependent transcription rate is determined by upstream emulator\n",
    "\n",
    "    \"\"\"                 \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_int: int = 5,\n",
    "        alpha_unconstr_init: torch.Tensor = None,\n",
    "        W: torch.Tensor = (torch.FloatTensor(5, 5).uniform_() > 0.5).int(),\n",
    "        W_int: torch.Tensor = None,\n",
    "        log_h_int: torch.Tensor = None,\n",
    "        global_time: bool = False,\n",
    "    ):\n",
    "        device = W.device\n",
    "        super().__init__()\n",
    "        self.n_int = n_int\n",
    "        if global_time:\n",
    "            self.log_h = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.log_phi = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.tau = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.o = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "        else:\n",
    "            self.log_h = torch.nn.Parameter(log_h_int.repeat(W.shape[0],1)*W)\n",
    "            self.log_phi = torch.nn.Parameter(torch.ones(W.shape).to(device)*W)\n",
    "            self.tau = torch.nn.Parameter(torch.ones(W.shape).to(device)*W*10)\n",
    "            self.o = torch.nn.Parameter(torch.ones(W.shape).to(device)*W)\n",
    "\n",
    "        self.mask_m = W\n",
    "        self.global_time = global_time\n",
    "\n",
    "        ## initialize grn\n",
    "        self.grn = torch.nn.Parameter(W_int*self.mask_m)\n",
    "        \n",
    "        ## initilize gamma and beta\n",
    "        self.beta_mean_unconstr = torch.nn.Parameter(0.5 * torch.ones(n_int))\n",
    "        self.gamma_mean_unconstr = torch.nn.Parameter(-1 * torch.ones(n_int))\n",
    "        self.alpha_unconstr_bias = torch.nn.Parameter(torch.zeros(n_int))\n",
    "        self.alpha_unconstr_max = torch.nn.Parameter(torch.randn(n_int))\n",
    "        # calculating emulating matrix\n",
    "    \n",
    "    ### define hook to froze the parameters\n",
    "    def _set_mask_grad(self):\n",
    "        self.hooks_grn = []\n",
    "        if not self.global_time:\n",
    "            self.hooks_log_h = []\n",
    "            self.hooks_log_phi = []\n",
    "            self.hooks_tau = []\n",
    "            self.hooks_o = []\n",
    "        #mask_m = self.mask_m\n",
    "        \n",
    "        def _hook_mask_no_regulator(grad):\n",
    "            return grad * self.mask_m\n",
    "\n",
    "        w_grn = self.grn.register_hook(_hook_mask_no_regulator)\n",
    "        self.hooks_grn.append(w_grn)\n",
    "        if not self.global_time:\n",
    "            w_log_h = self.log_h.register_hook(_hook_mask_no_regulator)\n",
    "            w_log_phi = self.log_phi.register_hook(_hook_mask_no_regulator)\n",
    "            w_tau = self.tau.register_hook(_hook_mask_no_regulator)\n",
    "            w_o = self.o.register_hook(_hook_mask_no_regulator)\n",
    "\n",
    "            self.hooks_log_h.append(w_log_h)\n",
    "            self.hooks_log_phi.append(w_log_phi)\n",
    "            self.hooks_tau.append(w_tau)\n",
    "            self.hooks_o.append(w_o)\n",
    "\n",
    "    def emulator(t,log_h_v,log_phi_v,tau_v,o_v):\n",
    "        pre = torch.exp(log_h_v)*torch.exp(-torch.exp(phi_v)*(t-tau_v)**2)+o_v\n",
    "        \n",
    "        return pre\n",
    "\n",
    "    def emulation_all(self,t: torch.Tensor = None):\n",
    "        if self.global_time:\n",
    "            # broadcasting the time t\n",
    "            t = t.repeat((self.mask_m.shape[0],1))\n",
    "\n",
    "        emulate_m = torch.zeros([self.mask_m.shape[0], self.mask_m.shape[1], t.shape[1]])\n",
    "\n",
    "        h = torch.exp(self.log_h)\n",
    "        phi = torch.exp(self.log_phi)\n",
    "        for i in range(t.shape[1]):\n",
    "            # for each time stamps, predict the emulator predict value\n",
    "            tt = t[:,i]\n",
    "            emu = h * torch.exp(-phi*(tt.reshape((len(tt),1))-self.tau)**2) + self.o\n",
    "            emulate_m[:,:,i] = emu\n",
    "\n",
    "        return emulate_m\n",
    "\n",
    "\n",
    "    def forward(self,t,g):\n",
    "        ## Encode \n",
    "\n",
    "        if self.global_time:\n",
    "            u = u[locate]\n",
    "            s = s[locate]\n",
    "            ## when use global time, t is a single value\n",
    "            T = t.repeat((dim,1))\n",
    "\n",
    "            ## calculate emulator vector\n",
    "            h = torch.exp(self.log_h)\n",
    "            phi = torch.exp(self.log_phi)\n",
    "            emu = h[locate,:] * torch.exp(-phi[locate,:]*(T.reshape((dim,1))-self.tau[locate,:])**2) + self.o[locate,:]\n",
    "            \n",
    "            ## Use the Emulator matrix to predict alpha\n",
    "            emu = emu * self.grn[locate,:]\n",
    "            alpha_unconstr = emu.sum(dim=1)\n",
    "            alpha_unconstr = alpha_unconstr + self.alpha_unconstr[locate]\n",
    "            \n",
    "            ## Generate kinetic rate\n",
    "            beta = torch.clamp(F.softplus(self.beta_mean_unconstr[locate]), 0, 50)\n",
    "            gamma = torch.clamp(F.softplus(self.gamma_mean_unconstr[locate]), 0, 50)\n",
    "            alpha = torch.clamp(F.softplus(alpha_unconstr),0,50)\n",
    "\n",
    "            ## Predict velocity\n",
    "            du = alpha - beta*u\n",
    "            ds = beta*u - gamma*s\n",
    "\n",
    "            du = du.reshape((dim,1))\n",
    "            ds = ds.reshape((dim,1))\n",
    "\n",
    "            v = torch.concatenate([du,ds],axis = 1)\n",
    "\n",
    "        else:\n",
    "            ## calculate emulator value\n",
    "            ## output the f_g(t)\n",
    "            \n",
    "            ## Build Emulator matrix for gene g\n",
    "            \n",
    "            h = torch.exp(self.log_h)[g,:].view(-1)\n",
    "            phi = torch.exp(self.log_phi)[g,:].view(-1)\n",
    "            tau = self.tau[g,:].view(-1)\n",
    "            o = self.o[g,:].view(-1)\n",
    "            w = self.grn[g,:].view(-1)\n",
    "            bias = self.alpha_unconstr_bias[g]\n",
    "            print(tau)\n",
    "            #emu = h[locate,:] * torch.exp(-phi[locate,:]*(T.reshape((dim,1))-self.tau[locate,:])**2) + self.o[locate,:]\n",
    "            emu = h * torch.exp(-phi*(t - tau)**2) + o\n",
    "\n",
    "            ## Use the Emulator matrix to predict alpha\n",
    "            #emu = emu * self.grn[locate,:]\n",
    "            emu = emu * w\n",
    "            \n",
    "            alpha_unconstr = emu.sum()\n",
    "            #alpha_unconstr = alpha_unconstr + self.alpha_unconstr_bias[locate]\n",
    "            alpha_unconstr = alpha_unconstr + bias\n",
    "\n",
    "            ## Generate transcription kinetic rate for time t\n",
    "            alpha = torch.clamp(alpha_unconstr,0,)\n",
    "            alpha = F.softsign(alpha)\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "\n",
    "\n",
    "class alpha_encoder2(nn.Module):\n",
    "    \"\"\" \n",
    "    encode the time dependent alpha (f)\n",
    "    time dependent transcription rate is determined by upstream emulator\n",
    "\n",
    "    \"\"\"                 \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_int: int = 5,\n",
    "        alpha_unconstr_init: torch.Tensor = None,\n",
    "        W: torch.Tensor = (torch.FloatTensor(5, 5).uniform_() > 0.5).int(),\n",
    "        W_int: torch.Tensor = None,\n",
    "        log_h_int: torch.Tensor = None,\n",
    "        global_time: bool = False,\n",
    "    ):\n",
    "        device = W.device\n",
    "        super().__init__()\n",
    "        self.n_int = n_int\n",
    "        self.device = device\n",
    "        if global_time:\n",
    "            self.log_h = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.log_phi = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.tau = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.o = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "        else:\n",
    "            self.log_h = torch.nn.Parameter(log_h_int.repeat(W.shape[0],1)*W)\n",
    "            self.log_phi = torch.nn.Parameter(torch.ones(W.shape).to(device)*W)\n",
    "            self.tau = torch.nn.Parameter(torch.ones(W.shape).to(device)*W*10)\n",
    "            self.o = torch.nn.Parameter(torch.ones(W.shape).to(device)*W)\n",
    "\n",
    "        self.mask_m = W\n",
    "        self.global_time = global_time\n",
    "\n",
    "        ## initialize grn\n",
    "        self.grn = torch.nn.Parameter(W_int*self.mask_m)\n",
    "        \n",
    "        ## initilize gamma and beta\n",
    "        self.beta_mean_unconstr = torch.nn.Parameter(0.5 * torch.ones(n_int).to(device))\n",
    "        self.gamma_mean_unconstr = torch.nn.Parameter(-1 * torch.ones(n_int).to(device))\n",
    "        self.alpha_unconstr_bias = torch.nn.Parameter(torch.zeros(n_int).to(device))\n",
    "        self.alpha_unconstr_max = torch.nn.Parameter(torch.randn(n_int).to(device))\n",
    "        # calculating emulating matrix\n",
    "    \n",
    "    ### define hook to froze the parameters\n",
    "    def _set_mask_grad(self):\n",
    "        self.hooks_grn = []\n",
    "        if not self.global_time:\n",
    "            self.hooks_log_h = []\n",
    "            self.hooks_log_phi = []\n",
    "            self.hooks_tau = []\n",
    "            self.hooks_o = []\n",
    "        #mask_m = self.mask_m\n",
    "        \n",
    "        def _hook_mask_no_regulator(grad):\n",
    "            return grad * self.mask_m\n",
    "\n",
    "        w_grn = self.grn.register_hook(_hook_mask_no_regulator)\n",
    "        self.hooks_grn.append(w_grn)\n",
    "        if not self.global_time:\n",
    "            w_log_h = self.log_h.register_hook(_hook_mask_no_regulator)\n",
    "            w_log_phi = self.log_phi.register_hook(_hook_mask_no_regulator)\n",
    "            w_tau = self.tau.register_hook(_hook_mask_no_regulator)\n",
    "            w_o = self.o.register_hook(_hook_mask_no_regulator)\n",
    "\n",
    "            self.hooks_log_h.append(w_log_h)\n",
    "            self.hooks_log_phi.append(w_log_phi)\n",
    "            self.hooks_tau.append(w_tau)\n",
    "            self.hooks_o.append(w_o)\n",
    "\n",
    "    def emulator(t,log_h_v,log_phi_v,tau_v,o_v):\n",
    "        pre = torch.exp(log_h_v)*torch.exp(-torch.exp(phi_v)*(t-tau_v)**2)+o_v\n",
    "        \n",
    "        return pre\n",
    "\n",
    "    def emulation_all(self,t: torch.Tensor = None):\n",
    "        if self.global_time:\n",
    "            # broadcasting the time t\n",
    "            t = t.repeat((self.mask_m.shape[0],1))\n",
    "\n",
    "        emulate_m = torch.zeros([self.mask_m.shape[0], self.mask_m.shape[1], t.shape[1]])\n",
    "\n",
    "        h = torch.exp(self.log_h)\n",
    "        phi = torch.exp(self.log_phi)\n",
    "        for i in range(t.shape[1]):\n",
    "            # for each time stamps, predict the emulator predict value\n",
    "            tt = t[:,i]\n",
    "            emu = h * torch.exp(-phi*(tt.reshape((len(tt),1))-self.tau)**2) + self.o\n",
    "            emulate_m[:,:,i] = emu\n",
    "\n",
    "        return emulate_m\n",
    "\n",
    "\n",
    "    def forward(self,t,t_scale,t0):\n",
    "        ## Encode \n",
    "\n",
    "        if self.global_time:\n",
    "            u = u[locate]\n",
    "            s = s[locate]\n",
    "            ## when use global time, t is a single value\n",
    "            T = t.repeat((dim,1))\n",
    "\n",
    "            ## calculate emulator vector\n",
    "            h = torch.exp(self.log_h)\n",
    "            phi = torch.exp(self.log_phi)\n",
    "            emu = h[locate,:] * torch.exp(-phi[locate,:]*(T.reshape((dim,1))-self.tau[locate,:])**2) + self.o[locate,:]\n",
    "            \n",
    "            ## Use the Emulator matrix to predict alpha\n",
    "            emu = emu * self.grn[locate,:]\n",
    "            alpha_unconstr = emu.sum(dim=1)\n",
    "            alpha_unconstr = alpha_unconstr + self.alpha_unconstr[locate]\n",
    "            \n",
    "            ## Generate kinetic rate\n",
    "            beta = torch.clamp(F.softplus(self.beta_mean_unconstr[locate]), 0, 50)\n",
    "            gamma = torch.clamp(F.softplus(self.gamma_mean_unconstr[locate]), 0, 50)\n",
    "            alpha = torch.clamp(F.softplus(alpha_unconstr),0,50)\n",
    "\n",
    "            ## Predict velocity\n",
    "            du = alpha - beta*u\n",
    "            ds = beta*u - gamma*s\n",
    "\n",
    "            du = du.reshape((dim,1))\n",
    "            ds = ds.reshape((dim,1))\n",
    "\n",
    "            v = torch.concatenate([du,ds],axis = 1)\n",
    "\n",
    "        else:\n",
    "            ## calculate emulator value\n",
    "            ## output the f_g(t)\n",
    "            \n",
    "            ## Build Emulator matrix for gene g\n",
    "            t = t.to(self.device)\n",
    "            h = torch.exp(self.log_h)\n",
    "            phi = torch.exp(self.log_phi)\n",
    "            tau = self.tau\n",
    "            o = self.o\n",
    "            w = self.grn\n",
    "            bias = self.alpha_unconstr_bias\n",
    "\n",
    "            #emu = h[locate,:] * torch.exp(-phi[locate,:]*(T.reshape((dim,1))-self.tau[locate,:])**2) + self.o[locate,:]\n",
    "            rt = t*t_scale+t0\n",
    "            if len(rt.shape) == 1:\n",
    "                rt = rt.reshape(1,-1)\n",
    "\n",
    "            ### create new dimensions for parameters\n",
    "            phi_rep = phi.repeat(rt.shape[0], 1, 1)\n",
    "            phi_rep = phi_rep.view(rt.shape[0], phi.shape[0], phi.shape[1])\n",
    "            \n",
    "            o_rep = o.repeat(rt.shape[0], 1, 1)\n",
    "            o_rep = o_rep.view(rt.shape[0], o.shape[0], o.shape[1])\n",
    "            \n",
    "            h_rep = h.repeat(rt.shape[0], 1, 1)\n",
    "            h_rep = h_rep.view(rt.shape[0], h.shape[0], h.shape[1])\n",
    "            \n",
    "            tau_rep = tau.repeat(rt.shape[0], 1, 1)\n",
    "            tau_rep = tau_rep.view(rt.shape[0], tau.shape[0], tau.shape[1])\n",
    "            \n",
    "            emu = h_rep * torch.exp(-phi_rep*(rt.unsqueeze(2) - tau_rep)**2) + o_rep\n",
    "            \n",
    "            ## Use the Emulator matrix to predict alpha\n",
    "            #emu = emu * self.grn[locate,:]\n",
    "            \n",
    "            w_rep = w.repeat(rt.shape[0], 1, 1)\n",
    "            w_rep = w_rep.view(rt.shape[0], w.shape[0], w.shape[1])\n",
    "            emu = emu * w_rep\n",
    "            \n",
    "            alpha_unconstr = emu.sum(dim=2)\n",
    "            #alpha_unconstr = alpha_unconstr + self.alpha_unconstr_bias[locate]\n",
    "            bias_rep = bias.repeat(rt.shape[0], 1)\n",
    "            alpha_unconstr = alpha_unconstr + bias_rep\n",
    "\n",
    "            ## Generate transcription kinetic rate for time t\n",
    "            alpha = torch.clamp(alpha_unconstr,0,)\n",
    "            alpha = F.softsign(alpha)\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "def SolveInitialValueProblem(f_t, x0, t0, t_eval):\n",
    "    \n",
    "    ## generate the prediction of unspliced/spliced readout at time t for every gene\n",
    "    ## use torchquad integral, different with torchode, the t_eval no longer need to be ordered\n",
    "    ## Now t is the G genes time in one cell\n",
    "    \n",
    "    ## get the kinetic parameters\n",
    "    beta = torch.clamp(F.softplus(f_t.beta_mean_unconstr), 0, 50)\n",
    "    gamma = torch.clamp(F.softplus(f_t.gamma_mean_unconstr), 0, 50)\n",
    "    alpha_max = torch.clamp(F.softplus(f_t.alpha_unconstr_max),0, 50)\n",
    "\n",
    "    ## define integral function\n",
    "    def integral_operator(tt,f_t, t0, beta, gamma):\n",
    "        t_scale = tt - t0 # when t0 is non-zero, minus t0 to make sure time start from 0\n",
    "        f_beta = lambda t: f_t(t,t_scale,t0)*torch.exp(beta*(t*t_scale+t0))*t_scale\n",
    "        f_gamma = lambda t: f_t(t,t_scale,t0)*torch.exp(gamma*(t*t_scale+t0))*t_scale\n",
    "        integration_domain = [[0,1]]\n",
    "        int_beta = simp.integrate(f_beta, dim=1, N=101, integration_domain=integration_domain)\n",
    "        int_gamma = simp.integrate(f_gamma, dim=1, N=101, integration_domain=integration_domain)\n",
    "        return torch.cat((int_beta,int_gamma))\n",
    "        \n",
    "    #def integral_alpha_gamma(tt,f_t,t0, gamma):\n",
    "    #    t_scale = tt - t0 # when t0 is non-zero, minus t0 to make sure time start from 0\n",
    "    #    f_i = lambda t: f_t(t,t_scale,t0)*torch.exp(gamma*t)\n",
    "    #    integration_domain = [[0,1]]\n",
    "    #    result = simp.integrate(f_i, dim=1, N=101, integration_domain=integration_domain)\n",
    "    #    return result\n",
    "\n",
    "    ## get the initial condition (i.e. u,s = 0,0)\n",
    "    u0 = x0[:,0]\n",
    "    s0 = x0[:,1]\n",
    "    \n",
    "    ## generate readout for each target gene\n",
    "    ## calculate integral for gene g\n",
    "    \n",
    "    integral = list(map(lambda tt: integral_operator(tt=tt,f_t=f_t,t0=t0,beta = beta, gamma = gamma), t_eval))\n",
    "    #integral = [integral_operator(tt=t_eval[i,:],f_t=f_t,t0=t0,beta = beta, gamma = gamma) for i in range(t_eval.shape[0])]\n",
    "    integral = torch.stack(integral)\n",
    "    integral_tensor_alpha_beta = integral[:,:t_eval.shape[1]]\n",
    "    integral_tensor_alpha_gamma = integral[:,t_eval.shape[1]:]\n",
    "    \n",
    "    pre_u = u0*torch.exp(-beta*t_eval) + alpha_max*torch.exp(-beta*t_eval)*integral_tensor_alpha_beta\n",
    "    pre_s = s0*torch.exp(-gamma*t_eval) + \\\n",
    "        ( (alpha_max*beta)/(gamma - beta) )*(torch.exp(-beta*t_eval)*integral_tensor_alpha_beta - torch.exp(-gamma*t_eval)*integral_tensor_alpha_gamma) + \\\n",
    "        ( (beta*u0)/(gamma - beta) )*(torch.exp(-beta*t_eval) - torch.exp(-gamma*t_eval))\n",
    "\n",
    "        \n",
    "    return pre_u,pre_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d9bc7a5-93cd-4c91-a1ba-5c82abd088db",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_t2 = alpha_encoder2(n_int = rgv_m.model.n_targets, alpha_unconstr_init = rgv_m.model.f_t.alpha_unconstr_bias,log_h_int = rgv_m.model.f_t.log_h[0,:],\n",
    "                    W = W.T, W_int = rgv_m.model.f_t.grn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae74f0c6-c8d1-47d5-81a6-c3240a0e71c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "t = m(torch.randn(30,250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9babc6a3-e262-48b8-97b1-d7d9c8d3fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e0f241b1-06c2-4822-8b02-7da82ad90b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "68570509-f1f0-4e8f-9926-7cdf4ad181d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.zeros((250,2))\n",
    "t0 = torch.zeros((250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "50e3b3e1-d26e-475f-9df8-31900b2a2a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.396217\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "starttime = datetime.datetime.now()\n",
    "u2,s2 = SolveInitialValueProblem(f_t2,x0,t0,t)\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b20e3b4-5b73-45f5-a86c-44673ca906b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2129, 0.2691, 0.2908,  ..., 0.0000, 0.0591, 0.1418],\n",
       "        [0.2549, 0.2802, 0.3133,  ..., 0.0000, 0.0591, 0.1597],\n",
       "        [0.3160, 0.2574, 0.3120,  ..., 0.0000, 0.0589, 0.2037],\n",
       "        ...,\n",
       "        [0.2501, 0.2744, 0.4935,  ..., 0.0000, 0.0604, 0.1512],\n",
       "        [0.2348, 0.3037, 0.3112,  ..., 0.0000, 0.0593, 0.1512],\n",
       "        [0.2126, 0.2720, 0.3122,  ..., 0.0000, 0.0848, 0.1759]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55897e4a-572c-407e-8210-553956e263fb",
   "metadata": {},
   "source": [
    "# use broadcast to 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "efc33cd5-6ada-4998-895d-2472fa4725fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoray import numpy as anp\n",
    "from loguru import logger\n",
    "from autoray import infer_backend\n",
    "from torchquad.integration.utils import _setup_integration_domain\n",
    "import torchquad as tq\n",
    "\n",
    "class alpha_encoder3(nn.Module):\n",
    "    \"\"\" \n",
    "    encode the time dependent alpha (f)\n",
    "    time dependent transcription rate is determined by upstream emulator\n",
    "\n",
    "    \"\"\"                 \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_int: int = 5,\n",
    "        alpha_unconstr_init: torch.Tensor = None,\n",
    "        W: torch.Tensor = (torch.FloatTensor(5, 5).uniform_() > 0.5).int(),\n",
    "        W_int: torch.Tensor = None,\n",
    "        log_h_int: torch.Tensor = None,\n",
    "        global_time: bool = False,\n",
    "    ):\n",
    "        device = W.device\n",
    "        super().__init__()\n",
    "        self.n_int = n_int\n",
    "        if global_time:\n",
    "            self.log_h = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.log_phi = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.tau = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.o = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "        else:\n",
    "            self.log_h = torch.nn.Parameter(log_h_int.repeat(W.shape[0],1)*W)\n",
    "            self.log_phi = torch.nn.Parameter(torch.ones(W.shape).to(device)*W)\n",
    "            self.tau = torch.nn.Parameter(torch.ones(W.shape).to(device)*W*10)\n",
    "            self.o = torch.nn.Parameter(torch.ones(W.shape).to(device)*W)\n",
    "\n",
    "        self.mask_m = W\n",
    "        self.global_time = global_time\n",
    "        self.device = device\n",
    "        ## initialize grn\n",
    "        self.grn = torch.nn.Parameter(W_int*self.mask_m)\n",
    "        \n",
    "        ## initilize gamma and beta\n",
    "        self.beta_mean_unconstr = torch.nn.Parameter(0.5 * torch.ones(n_int).to(device))\n",
    "        self.gamma_mean_unconstr = torch.nn.Parameter(-1 * torch.ones(n_int).to(device))\n",
    "        self.alpha_unconstr_bias = torch.nn.Parameter(torch.zeros(n_int).to(device))\n",
    "        self.alpha_unconstr_max = torch.nn.Parameter(torch.randn(n_int).to(device))\n",
    "        \n",
    "        # create integrator\n",
    "        integration_domain = [[0,1]]\n",
    "        simp = tq.Simpson()\n",
    "        integration_domain = _setup_integration_domain(dim=1, integration_domain=integration_domain, backend=\"torch\")\n",
    "        grid_points, hs, n_per_dim = simp.calculate_grid(N=101, integration_domain=integration_domain)\n",
    "        grid_points = grid_points.to(mps_device)\n",
    "        self.grid_points = grid_points\n",
    "        self.hs = hs\n",
    "        self.n_per_dim = n_per_dim\n",
    "    \n",
    "    ### define hook to froze the parameters\n",
    "    def _set_mask_grad(self):\n",
    "        self.hooks_grn = []\n",
    "        if not self.global_time:\n",
    "            self.hooks_log_h = []\n",
    "            self.hooks_log_phi = []\n",
    "            self.hooks_tau = []\n",
    "            self.hooks_o = []\n",
    "        #mask_m = self.mask_m\n",
    "        \n",
    "        def _hook_mask_no_regulator(grad):\n",
    "            return grad * self.mask_m\n",
    "\n",
    "        w_grn = self.grn.register_hook(_hook_mask_no_regulator)\n",
    "        self.hooks_grn.append(w_grn)\n",
    "        if not self.global_time:\n",
    "            w_log_h = self.log_h.register_hook(_hook_mask_no_regulator)\n",
    "            w_log_phi = self.log_phi.register_hook(_hook_mask_no_regulator)\n",
    "            w_tau = self.tau.register_hook(_hook_mask_no_regulator)\n",
    "            w_o = self.o.register_hook(_hook_mask_no_regulator)\n",
    "\n",
    "            self.hooks_log_h.append(w_log_h)\n",
    "            self.hooks_log_phi.append(w_log_phi)\n",
    "            self.hooks_tau.append(w_tau)\n",
    "            self.hooks_o.append(w_o)\n",
    "\n",
    "    def emulator(t,log_h_v,log_phi_v,tau_v,o_v):\n",
    "        pre = torch.exp(log_h_v)*torch.exp(-torch.exp(phi_v)*(t-tau_v)**2)+o_v\n",
    "        \n",
    "        return pre\n",
    "\n",
    "    def emulation_all(self,t: torch.Tensor = None):\n",
    "        if self.global_time:\n",
    "            # broadcasting the time t\n",
    "            t = t.repeat((self.mask_m.shape[0],1))\n",
    "\n",
    "        emulate_m = torch.zeros([self.mask_m.shape[0], self.mask_m.shape[1], t.shape[1]])\n",
    "\n",
    "        h = torch.exp(self.log_h)\n",
    "        phi = torch.exp(self.log_phi)\n",
    "        for i in range(t.shape[1]):\n",
    "            # for each time stamps, predict the emulator predict value\n",
    "            tt = t[:,i]\n",
    "            emu = h * torch.exp(-phi*(tt.reshape((len(tt),1))-self.tau)**2) + self.o\n",
    "            emulate_m[:,:,i] = emu\n",
    "\n",
    "        return emulate_m\n",
    "\n",
    "\n",
    "    def forward(self,t,t_scale,t0):\n",
    "        ## Encode \n",
    "\n",
    "        if self.global_time:\n",
    "            u = u[locate]\n",
    "            s = s[locate]\n",
    "            ## when use global time, t is a single value\n",
    "            T = t.repeat((dim,1))\n",
    "\n",
    "            ## calculate emulator vector\n",
    "            h = torch.exp(self.log_h)\n",
    "            phi = torch.exp(self.log_phi)\n",
    "            emu = h[locate,:] * torch.exp(-phi[locate,:]*(T.reshape((dim,1))-self.tau[locate,:])**2) + self.o[locate,:]\n",
    "            \n",
    "            ## Use the Emulator matrix to predict alpha\n",
    "            emu = emu * self.grn[locate,:]\n",
    "            alpha_unconstr = emu.sum(dim=1)\n",
    "            alpha_unconstr = alpha_unconstr + self.alpha_unconstr[locate]\n",
    "            \n",
    "            ## Generate kinetic rate\n",
    "            sp = nn.Softplus()\n",
    "            beta = torch.clamp(sp(self.beta_mean_unconstr[locate]), 0, 50)\n",
    "            gamma = torch.clamp(sp(self.gamma_mean_unconstr[locate]), 0, 50)\n",
    "            alpha = torch.clamp(sp(alpha_unconstr),0,50)\n",
    "\n",
    "            ## Predict velocity\n",
    "            du = alpha - beta*u\n",
    "            ds = beta*u - gamma*s\n",
    "\n",
    "            du = du.reshape((dim,1))\n",
    "            ds = ds.reshape((dim,1))\n",
    "\n",
    "            v = torch.concatenate([du,ds],axis = 1)\n",
    "\n",
    "        else:\n",
    "            ## calculate emulator value\n",
    "            ## output the f_g(t)\n",
    "            \n",
    "            ## Build Emulator matrix for gene g\n",
    "            t = t.to(self.device)\n",
    "            h = torch.exp(self.log_h)\n",
    "            phi = torch.exp(self.log_phi)\n",
    "            tau = self.tau\n",
    "            o = self.o\n",
    "            w = self.grn\n",
    "            bias = self.alpha_unconstr_bias\n",
    "\n",
    "            #emu = h[locate,:] * torch.exp(-phi[locate,:]*(T.reshape((dim,1))-self.tau[locate,:])**2) + self.o[locate,:]\n",
    "            #rt = t*t_scale+t0\n",
    "            rt = torch.einsum('i,jk->ijk', t.view(-1), t_scale) + t0\n",
    "            \n",
    "            if len(rt.shape) == 2:\n",
    "                #rt = rt.reshape(1,-1)\n",
    "                rt = rt.unsqueeze(0)\n",
    "                \n",
    "            ### create new dimensions for parameters\n",
    "            phi = phi.expand(rt.shape[0], rt.shape[1],phi.shape[0], phi.shape[1])\n",
    "            o = o.expand(rt.shape[0], rt.shape[1],o.shape[0], o.shape[1])\n",
    "            h = h.expand(rt.shape[0], rt.shape[1],h.shape[0], h.shape[1])\n",
    "            tau = tau.expand(rt.shape[0], rt.shape[1],tau.shape[0], tau.shape[1])\n",
    "            \n",
    "            emu = h * torch.exp(-phi*(rt.unsqueeze(3) - tau)**2) + o\n",
    "            \n",
    "            ## Use the Emulator matrix to predict alpha\n",
    "            #emu = emu * self.grn[locate,:]\n",
    "            w = w.expand(rt.shape[0], rt.shape[1],w.shape[0], w.shape[1])\n",
    "            emu = emu * w\n",
    "            \n",
    "            alpha_unconstr = emu.sum(dim=3)\n",
    "            #alpha_unconstr = alpha_unconstr + self.alpha_unconstr_bias[locate]\n",
    "            bias = bias.expand(rt.shape[0], rt.shape[1], bias.shape[0])\n",
    "            alpha_unconstr = alpha_unconstr + bias\n",
    "\n",
    "            ## Generate transcription kinetic rate for time t\n",
    "            alpha = torch.clamp(alpha_unconstr,0,)\n",
    "            alpha = F.softsign(alpha)\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "def SolveInitialValueProblem(f_t, x0, t0, t_eval):\n",
    "    \n",
    "    ## generate the prediction of unspliced/spliced readout at time t for every gene\n",
    "    ## use torchquad integral, different with torchode, the t_eval no longer need to be ordered\n",
    "    ## Now t is the G genes time in one cell\n",
    "    \n",
    "    ## get the kinetic parameters\n",
    "    beta = torch.clamp(F.softplus(f_t.beta_mean_unconstr), 0, 50)\n",
    "    gamma = torch.clamp(F.softplus(f_t.gamma_mean_unconstr), 0, 50)\n",
    "    alpha_max = torch.clamp(F.softplus(f_t.alpha_unconstr_max),0, 50)\n",
    "    \n",
    "    def calculate_result(simp, function_values, dim, n_per_dim, hs):\n",
    "        \"\"\"Apply the Composite Newton Cotes rule to calculate a result from the evaluated integrand.\n",
    "        Args:\n",
    "            function_values (backend tensor): Output of the integrand\n",
    "            dim (int): Dimensionality\n",
    "            n_per_dim (int): Number of grid slices per dimension\n",
    "            hs (backend tensor): Distances between grid slices for each dimension\n",
    "        Returns:\n",
    "            backend tensor: Quadrature result\n",
    "        \"\"\"\n",
    "        # Reshape the output to be [integrand_dim,N,N,...] points instead of [integrand_dim,dim*N] points\n",
    "        integrand_shape = function_values.shape[1:]\n",
    "        dim_shape = [n_per_dim] * dim\n",
    "        new_shape = [*integrand_shape, *dim_shape]\n",
    "        # We need to use einsum instead of just reshape here because reshape does not move the axis - it only reshapes.\n",
    "        # So the first line generates a character string for einsum, followed by moving the first dimension i.e `dim*N`\n",
    "        # to the end.  Finally we reshape the resulting object so that instead of the last dimension being `dim*N`, it is\n",
    "        # `N,N,...` as desired.\n",
    "        einsum = \"\".join([chr(i + 65) for i in range(len(function_values.shape))])\n",
    "        reshaped_function_values = anp.einsum(\n",
    "            f\"{einsum}->{einsum[1:]}{einsum[0]}\", function_values\n",
    "        )\n",
    "        reshaped_function_values = reshaped_function_values.reshape(new_shape)\n",
    "        assert new_shape == list(\n",
    "            reshaped_function_values.shape\n",
    "        ), f\"reshaping produced shape {reshaped_function_values.shape}, expected shape was {new_shape}\"\n",
    "        logger.debug(\"Computing areas.\")\n",
    "\n",
    "        result = simp._apply_composite_rule(reshaped_function_values, dim, hs)\n",
    "\n",
    "        logger.opt(lazy=True).info(\n",
    "            \"Computed integral: {result}\", result=lambda: str(result)\n",
    "        )\n",
    "        return result\n",
    "\n",
    "    ## define integral function\n",
    "    u0 = x0[:,0]\n",
    "    s0 = x0[:,1]\n",
    "    \n",
    "    t_scale = t_eval - t0 # when t0 is non-zero, minus t0 to make sure time start from 0\n",
    "    f_beta = lambda t: f_t(t,t_scale,t0)*torch.exp(beta*((torch.einsum('i,jk->ijk', t.view(-1), t_scale) + t0)))*t_scale\n",
    "    f_gamma = lambda t: f_t(t,t_scale,t0)*torch.exp(gamma*((torch.einsum('i,jk->ijk', t.view(-1), t_scale) + t0)))*t_scale\n",
    "\n",
    "    function_values1, num_points = simp.evaluate_integrand(f_beta, f_t.grid_points)\n",
    "    function_values2, _ = simp.evaluate_integrand(f_gamma, f_t.grid_points)\n",
    "    simp._nr_of_fevals = num_points\n",
    "    integral_tensor_alpha_beta = calculate_result(simp,function_values1,1,f_t.n_per_dim,f_t.hs)\n",
    "    integral_tensor_alpha_gamma = calculate_result(simp,function_values2,1,f_t.n_per_dim,f_t.hs)\n",
    "        \n",
    "    #def integral_alpha_gamma(tt,f_t,t0, gamma):\n",
    "    #    t_scale = tt - t0 # when t0 is non-zero, minus t0 to make sure time start from 0\n",
    "    #    f_i = lambda t: f_t(t,t_scale,t0)*torch.exp(gamma*t)\n",
    "    #    integration_domain = [[0,1]]\n",
    "    #    result = simp.integrate(f_i, dim=1, N=101, integration_domain=integration_domain)\n",
    "    #    return result\n",
    "\n",
    "    ## get the initial condition (i.e. u,s = 0,0)\n",
    "    \n",
    "    ## generate readout for each target gene\n",
    "    ## calculate integral for gene g\n",
    "    \n",
    "    #integral = list(map(lambda tt: integral_operator(tt=tt,f_t=f_t,t0=t0,beta = beta, gamma = gamma), t_eval))\n",
    "    #integral = [integral_operator(tt=t_eval[i,:],f_t=f_t,t0=t0,beta = beta, gamma = gamma) for i in range(t_eval.shape[0])]\n",
    "    #integral = torch.stack(integral)\n",
    "    #integral_tensor_alpha_beta, integral_tensor_alpha_gamma = integral_operator(t_eval, f_t, t0 = t0, beta = beta, gamma = gamma)\n",
    "    \n",
    "    #pre_u = u0*torch.exp(-beta*t_eval) + alpha_max*torch.exp(-beta*t_eval)*integral_tensor_alpha_beta\n",
    "    #pre_s = s0*torch.exp(-gamma*t_eval) + \\\n",
    "    #    ( (alpha_max*beta)/(gamma - beta) )*(torch.exp(-beta*t_eval)*integral_tensor_alpha_beta - torch.exp(-gamma*t_eval)*integral_tensor_alpha_gamma) + \\\n",
    "    #    ( (beta*u0)/(gamma - beta) )*(torch.exp(-beta*t_eval) - torch.exp(-gamma*t_eval))\n",
    "\n",
    "        \n",
    "    return integral_tensor_alpha_beta, integral_tensor_alpha_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f25a6ef0-e0fc-4ab9-87b2-11e1fc9e58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_result(simp, function_values, dim, n_per_dim, hs):\n",
    "    \"\"\"Apply the Composite Newton Cotes rule to calculate a result from the evaluated integrand.\n",
    "    Args:\n",
    "        function_values (backend tensor): Output of the integrand\n",
    "        dim (int): Dimensionality\n",
    "        n_per_dim (int): Number of grid slices per dimension\n",
    "        hs (backend tensor): Distances between grid slices for each dimension\n",
    "    Returns:\n",
    "        backend tensor: Quadrature result\n",
    "    \"\"\"\n",
    "    # Reshape the output to be [integrand_dim,N,N,...] points instead of [integrand_dim,dim*N] points\n",
    "    integrand_shape = function_values.shape[1:]\n",
    "    dim_shape = [n_per_dim] * dim\n",
    "    new_shape = [*integrand_shape, *dim_shape]\n",
    "    # We need to use einsum instead of just reshape here because reshape does not move the axis - it only reshapes.\n",
    "    # So the first line generates a character string for einsum, followed by moving the first dimension i.e `dim*N`\n",
    "    # to the end.  Finally we reshape the resulting object so that instead of the last dimension being `dim*N`, it is\n",
    "    # `N,N,...` as desired.\n",
    "    einsum = \"\".join([chr(i + 65) for i in range(len(function_values.shape))])\n",
    "    reshaped_function_values = anp.einsum(\n",
    "        f\"{einsum}->{einsum[1:]}{einsum[0]}\", function_values\n",
    "    )\n",
    "    reshaped_function_values = reshaped_function_values.reshape(new_shape)\n",
    "    assert new_shape == list(\n",
    "        reshaped_function_values.shape\n",
    "    ), f\"reshaping produced shape {reshaped_function_values.shape}, expected shape was {new_shape}\"\n",
    "    logger.debug(\"Computing areas.\")\n",
    "\n",
    "    result = simp._apply_composite_rule(reshaped_function_values, dim, hs)\n",
    "\n",
    "    logger.opt(lazy=True).info(\n",
    "        \"Computed integral: {result}\", result=lambda: str(result)\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af05fe99-762d-4acd-bcfb-628a0a4bbc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "588be638-f7cf-4fc5-9e1e-99e66ac517d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_t3 = alpha_encoder3(n_int = rgv_m.model.n_targets, alpha_unconstr_init = rgv_m.model.f_t.alpha_unconstr_bias.to(mps_device),log_h_int = rgv_m.model.f_t.log_h[0,:].to(mps_device),\n",
    "                    W = W.T.to(mps_device), W_int = rgv_m.model.f_t.grn.to(mps_device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0d70cd8-0611-47c3-a7af-7b185ef2ad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = torch.zeros(t.shape).to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f06e863b-c2ed-466a-9718-61cb8c82f778",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = t.to(mps_device)\n",
    "x0 = torch.zeros((250,2)).to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcf8e3c7-9b84-4627-ba29-3bb3b670dabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.027449\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "starttime = datetime.datetime.now()\n",
    "u,s = SolveInitialValueProblem(f_t3, x0, t0, t)\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d323b8a-ed6f-4030-b8f3-13aac30df59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.057828\n"
     ]
    }
   ],
   "source": [
    "beta = torch.clamp(F.softplus(f_t3.beta_mean_unconstr), 0, 50)\n",
    "gamma = torch.clamp(F.softplus(f_t3.gamma_mean_unconstr), 0, 50)\n",
    "alpha_max = torch.clamp(F.softplus(f_t3.alpha_unconstr_max),0, 50)\n",
    "u0 = x0[:,0]\n",
    "s0 = x0[:,1]\n",
    "import datetime\n",
    "starttime = datetime.datetime.now()\n",
    "pre_u = u0*torch.exp(-beta*t) + alpha_max*torch.exp(-beta*t)*u\n",
    "pre_s = (beta/(gamma - beta))*pre_u + s0*torch.exp(-gamma*t) - ( (alpha_max*beta)/(gamma - beta) )*torch.exp(-gamma*t)*s - ( (beta*u0)/(gamma - beta) )*torch.exp(-gamma*t)\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6027719b-8d8b-4cca-a804-f78cc5955dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "986b5b72-87d8-4870-8f59-00ec6439571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_s = (beta/(gamma - beta))*pre_u + s0*torch.exp(-gamma*t) - ( (alpha_max*beta)/(gamma - beta) )*torch.exp(-gamma*t)*s - ( (beta*u0)/(gamma - beta) )*torch.exp(-gamma*t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f7777d50-09cc-415f-a3ef-5c912072c45a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8347, 1.7346, 1.2099,  ..., 0.0000, 0.0626, 0.1656],\n",
       "        [2.7806, 3.2880, 3.2434,  ..., 0.0000, 0.0726, 0.4338],\n",
       "        [2.4508, 1.2165, 2.3972,  ..., 0.0000, 0.0567, 0.3809],\n",
       "        ...,\n",
       "        [2.7734, 3.0816, 3.6759,  ..., 0.0000, 0.0763, 0.3489],\n",
       "        [2.7273, 3.5057, 2.2039,  ..., 0.0000, 0.0736, 0.3372],\n",
       "        [1.7256, 2.0781, 2.4848,  ..., 0.0000, 0.0743, 0.4364]],\n",
       "       device='mps:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbb6c3b7-cbf3-4dda-aec0-3abfdd7f69c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integral_operator(tt,f_t, t0, beta, gamma):\n",
    "    t_scale = tt - t0 # when t0 is non-zero, minus t0 to make sure time start from 0\n",
    "    f_beta = lambda t: f_t(t,t_scale,t0)*torch.exp(beta*((torch.einsum('i,jk->ijk', t.to(mps_device).view(-1), t_scale) + t0)))*t_scale\n",
    "    f_gamma = lambda t: f_t(t,t_scale,t0)*torch.exp(gamma*((torch.einsum('i,jk->ijk', t.to(mps_device).view(-1), t_scale) + t0)))*t_scale\n",
    "    integration_domain = [[0,1]]\n",
    "    int_beta = simp.integrate(f_beta, dim=1, N=101, integration_domain=integration_domain,backend=\"torch\")\n",
    "    int_gamma = simp.integrate(f_gamma, dim=1, N=101, integration_domain=integration_domain,backend=\"torch\")\n",
    "    return int_beta, int_gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae5d4559-71ce-4ddd-8f85-e62414762291",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_scale = t - t0\n",
    "beta = torch.clamp(F.softplus(f_t3.beta_mean_unconstr), 0, 50)\n",
    "gamma = torch.clamp(F.softplus(f_t3.gamma_mean_unconstr), 0, 50)\n",
    "alpha_max = torch.clamp(F.softplus(f_t3.alpha_unconstr_max),0, 50)\n",
    "f_beta = lambda t: f_t3(t,t_scale,t0)*torch.exp(beta*((torch.einsum('i,jk->ijk', t.view(-1), t_scale) + t0)))*t_scale\n",
    "f_gamma = lambda t: f_t3(t,t_scale,t0)*torch.exp(gamma*((torch.einsum('i,jk->ijk', t.view(-1), t_scale) + t0)))*t_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8cdde1ce-6c1a-4219-9833-c7baa7933c3d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 14.23 GB, other allocations: 3.90 GB, max allowed: 18.13 GB). Tried to allocate 390.10 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m starttime \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[1;32m      5\u001b[0m function_values1, num_points \u001b[38;5;241m=\u001b[39m simp\u001b[38;5;241m.\u001b[39mevaluate_integrand(f_beta, f_t3\u001b[38;5;241m.\u001b[39mgrid_points)\n\u001b[0;32m----> 6\u001b[0m function_values2, num_points \u001b[38;5;241m=\u001b[39m \u001b[43msimp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_integrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_gamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf_t3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrid_points\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m simp\u001b[38;5;241m.\u001b[39m_nr_of_fevals \u001b[38;5;241m=\u001b[39m num_points\n\u001b[1;32m      8\u001b[0m integral_tensor_alpha_beta \u001b[38;5;241m=\u001b[39m calculate_result(simp,function_values1,\u001b[38;5;241m1\u001b[39m,f_t3\u001b[38;5;241m.\u001b[39mn_per_dim,f_t3\u001b[38;5;241m.\u001b[39mhs)\n",
      "File \u001b[0;32m~/miniconda3/envs/RegVelo/lib/python3.10/site-packages/torchquad/integration/base_integrator.py:55\u001b[0m, in \u001b[0;36mBaseIntegrator.evaluate_integrand\u001b[0;34m(fn, points)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Evaluate the integrand function at the passed points\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    int: Number of evaluated points\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m num_points \u001b[38;5;241m=\u001b[39m points\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 55\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpoints\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m infer_backend(result) \u001b[38;5;241m!=\u001b[39m infer_backend(points):\n\u001b[1;32m     57\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe passed function\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms return value has a different numerical backend than the passed points. Will try to convert. Note that this may be slow as it results in memory transfers between CPU and GPU, if torchquad uses the GPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[33], line 6\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m      4\u001b[0m alpha_max \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(F\u001b[38;5;241m.\u001b[39msoftplus(f_t3\u001b[38;5;241m.\u001b[39malpha_unconstr_max),\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      5\u001b[0m f_beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t: f_t3(t,t_scale,t0)\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mexp(beta\u001b[38;5;241m*\u001b[39m((torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi,jk->ijk\u001b[39m\u001b[38;5;124m'\u001b[39m, t\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), t_scale) \u001b[38;5;241m+\u001b[39m t0)))\u001b[38;5;241m*\u001b[39mt_scale\n\u001b[0;32m----> 6\u001b[0m f_gamma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mf_t3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43mt0\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m*\u001b[39mtorch\u001b[38;5;241m.\u001b[39mexp(gamma\u001b[38;5;241m*\u001b[39m((torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi,jk->ijk\u001b[39m\u001b[38;5;124m'\u001b[39m, t\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m), t_scale) \u001b[38;5;241m+\u001b[39m t0)))\u001b[38;5;241m*\u001b[39mt_scale\n",
      "File \u001b[0;32m~/miniconda3/envs/RegVelo/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[26], line 168\u001b[0m, in \u001b[0;36malpha_encoder3.forward\u001b[0;34m(self, t, t_scale, t0)\u001b[0m\n\u001b[1;32m    165\u001b[0m h \u001b[38;5;241m=\u001b[39m h\u001b[38;5;241m.\u001b[39mexpand(rt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], rt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],h\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], h\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    166\u001b[0m tau \u001b[38;5;241m=\u001b[39m tau\u001b[38;5;241m.\u001b[39mexpand(rt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], rt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],tau\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], tau\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m--> 168\u001b[0m emu \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;241m-\u001b[39mphi\u001b[38;5;241m*\u001b[39m(\u001b[43mrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m o\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m## Use the Emulator matrix to predict alpha\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m#emu = emu * self.grn[locate,:]\u001b[39;00m\n\u001b[1;32m    172\u001b[0m w \u001b[38;5;241m=\u001b[39m w\u001b[38;5;241m.\u001b[39mexpand(rt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], rt\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],w\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], w\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 14.23 GB, other allocations: 3.90 GB, max allowed: 18.13 GB). Tried to allocate 390.10 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "u0 = x0[:,0]\n",
    "s0 = x0[:,1]\n",
    "starttime = datetime.datetime.now()\n",
    "function_values1, num_points = simp.evaluate_integrand(f_beta, f_t3.grid_points)\n",
    "function_values2, num_points = simp.evaluate_integrand(f_gamma, f_t3.grid_points)\n",
    "simp._nr_of_fevals = num_points\n",
    "integral_tensor_alpha_beta = calculate_result(simp,function_values1,1,f_t3.n_per_dim,f_t3.hs)\n",
    "integral_tensor_alpha_gamma = calculate_result(simp,function_values2,1,f_t3.n_per_dim,f_t3.hs)\n",
    "pre_u = u0*torch.exp(-beta*t) + alpha_max*torch.exp(-beta*t)*integral_tensor_alpha_beta\n",
    "pre_s = s0*torch.exp(-gamma*t) + \\\n",
    "    ( (alpha_max*beta)/(gamma - beta) )*(torch.exp(-beta*t)*integral_tensor_alpha_beta - torch.exp(-gamma*t)*integral_tensor_alpha_gamma) + \\\n",
    "    ( (beta*u0)/(gamma - beta) )*(torch.exp(-beta*t) - torch.exp(-gamma*t))\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2efad7af-51e3-42a0-b525-1ac7e3ea29d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.858688\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "starttime = datetime.datetime.now()\n",
    "integral_operator(t.to(mps_device),f_t3,t0.to(mps_device),beta,gamma)\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0819ac06-35c4-483e-9fcd-7f86b413aafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sample = m(torch.randn(101,1))\n",
    "t_scale = t\n",
    "t_sample = t_sample.to(f_t3.device)\n",
    "t_scale = t_scale.to(f_t3.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fcc30f94-3c7a-4767-81c2-6fee1bf97ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_beta = lambda t: f_t3(t,t_scale,t0)*torch.exp(beta*((torch.einsum('i,jk->ijk', t.to(mps_device).view(-1), t_scale) + t0)))*t_scale\n",
    "f_gamma = lambda t: f_t3(t,t_scale,t0)*torch.exp(gamma*((torch.einsum('i,jk->ijk', t.to(mps_device).view(-1), t_scale) + t0)))*t_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "94cdab97-1c65-4cd3-b049-e5b1a64cf3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.026669\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "starttime = datetime.datetime.now()\n",
    "f_beta(t_sample)\n",
    "f_gamma(t_sample)\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a9a2a68-01b2-4648-be56-0a97e4a0b10b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([250])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softplus(f_t3.beta_mean_unconstr.to(\"cpu\")).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2865cb89-6199-4101-94fe-6ef3caab3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sample = m(torch.randn(101,1))\n",
    "t_scale = t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "33e0dbf2-c016-4d55-836a-8126fcef1fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sample = t_sample.to(f_t3.device)\n",
    "t_scale = t_scale.to(f_t3.device)\n",
    "h = torch.exp(f_t3.log_h)\n",
    "phi = torch.exp(f_t3.log_phi)\n",
    "tau = f_t3.tau\n",
    "o = f_t3.o\n",
    "w = f_t3.grn\n",
    "bias = f_t3.alpha_unconstr_bias\n",
    "\n",
    "#emu = h[locate,:] * torch.exp(-phi[locate,:]*(T.reshape((dim,1))-self.tau[locate,:])**2) + self.o[locate,:]\n",
    "#rt = t*t_scale+t0\n",
    "rt = torch.einsum('i,jk->ijk', t_sample.view(-1), t_scale) + t0\n",
    "\n",
    "#if len(rt.shape) == 2:\n",
    "    #rt = rt.reshape(1,-1)\n",
    "#    rt = rt.unsqueeze(0)\n",
    "\n",
    "### create new dimensions for parameters\n",
    "phi = phi.expand(rt.shape[0], rt.shape[1],phi.shape[0], phi.shape[1])\n",
    "o = o.expand(rt.shape[0], rt.shape[1],o.shape[0], o.shape[1])\n",
    "h = h.expand(rt.shape[0], rt.shape[1],h.shape[0], h.shape[1])\n",
    "tau = tau.expand(rt.shape[0], rt.shape[1],tau.shape[0], tau.shape[1])\n",
    "\n",
    "emu = h * torch.exp(-phi*(rt.unsqueeze(3) - tau)**2) + o\n",
    "#emu = h_rep * torch.exp(-phi_rep*(rt.unsqueeze(3) - tau_rep)**2) + o_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36cb54ac-4168-4ce4-92d0-e9b1ac1cf439",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 14.64 GB, other allocations: 2.65 GB, max allowed: 18.13 GB). Tried to allocate 1.63 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m emu \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mphi\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m o\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 14.64 GB, other allocations: 2.65 GB, max allowed: 18.13 GB). Tried to allocate 1.63 GB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "emu = h * torch.exp(-phi*(rt.unsqueeze(3) - tau)**2) + o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "019de82d-68ed-4b31-bb50-d8a154f4616a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 6.5331e+03, 8.3326e+03,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00],\n",
       "        [1.0000e+00, 6.5331e+03, 1.0000e+00,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00],\n",
       "        [1.0000e+00, 6.5331e+03, 1.0000e+00,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00],\n",
       "        ...,\n",
       "        [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00],\n",
       "        [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00],\n",
       "        [1.0000e+00, 1.0000e+00, 1.0000e+00,  ..., 1.0000e+00, 1.0000e+00,\n",
       "         1.0000e+00]], device='mps:0', grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(f_t3.log_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4551681c-951d-4003-ac45-7eb7222014c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = phi.to(mps_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d7e59e01-4a8a-4e0d-9627-d3f4979c09a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = phi[None,None,:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fd9749aa-d6db-42bd-8a65-54946e5e22cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "         [[1.0000, 2.7183, 2.7183,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [2.7183, 2.7183, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]]]],\n",
       "       device='mps:0', grad_fn=<ExpandBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi.expand(t.shape[0], t.shape[1],phi.shape[2], phi.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e5b6f82b-73e0-4488-9143-02330af5c8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.009825\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "starttime = datetime.datetime.now()\n",
    "emu = h_rep * torch.exp(-phi_rep*(rt.unsqueeze(3) - tau_rep)**2) + o_rep\n",
    "\n",
    "w_rep = w.repeat(rt.shape[0], rt.shape[1], 1, 1)\n",
    "w_rep = w_rep.view(rt.shape[0], rt.shape[1], w.shape[0], w.shape[1])\n",
    "emu = emu * w_rep\n",
    "\n",
    "alpha_unconstr = emu.sum(dim=3)\n",
    "#alpha_unconstr = alpha_unconstr + self.alpha_unconstr_bias[locate]\n",
    "bias_rep = bias.repeat(rt.shape[0], rt.shape[1], 1)\n",
    "alpha_unconstr = alpha_unconstr + bias_rep\n",
    "\n",
    "## Generate transcription kinetic rate for time t\n",
    "alpha = torch.clamp(alpha_unconstr,0,)\n",
    "alpha = F.softsign(alpha)\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dc697770-1e16-4587-8ae0-59e0a19407dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_beta = lambda t: f_t3(t,t_scale,t0)*torch.exp(beta*((torch.einsum('i,jk->ijk', t.to(mps_device).view(-1), t_scale) + t0)))*t_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "385a3764-4798-4177-9f03-73da9b219bca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.4400e+05, 3.5495e+06, 1.8138e+07,  ..., 0.0000e+00,\n",
       "          7.0994e+06, 5.6101e+08],\n",
       "         [3.7764e+08, 2.2301e+13, 2.5238e+05,  ..., 0.0000e+00,\n",
       "          1.8676e+12, 8.8445e+09],\n",
       "         [8.6245e+11, 1.7863e+02, 6.5715e+10,  ..., 0.0000e+00,\n",
       "          4.8630e+04, 4.8691e+12],\n",
       "         ...,\n",
       "         [1.4334e+01, 1.1040e+10, 3.6286e+12,  ..., 0.0000e+00,\n",
       "          4.5279e+08, 1.1547e+09],\n",
       "         [4.2046e+08, 2.0325e+06, 2.7620e+07,  ..., 0.0000e+00,\n",
       "          9.9938e+14, 2.0619e+01],\n",
       "         [1.4722e+07, 5.5826e+04, 5.8399e+02,  ..., 0.0000e+00,\n",
       "          6.8258e+06, 8.2616e+00]]], device='mps:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_beta(torch.tensor(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acecb0fa-2e7d-4841-a973-492059140dd4",
   "metadata": {},
   "source": [
    "# Compare with torchode solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c41054-84b4-4736-9a36-c091f0ac5a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e9b17-fecc-46ca-aeac-073ca57cd1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4eb215-53b4-4973-b1e9-26b514ee7165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "299d3c3d-745c-41cc-b758-06d8103bd3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class velocity_encoder(nn.Module):\n",
    "    \"\"\" \n",
    "    encode the velocity\n",
    "    time dependent transcription rate is determined by upstream emulator\n",
    "    velocity could be build on top of this\n",
    "\n",
    "    merge velocity encoder and emulator class\n",
    "    \"\"\"                 \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_int: int = 5,\n",
    "        alpha_unconstr_init: torch.Tensor = None,\n",
    "        W: torch.Tensor = (torch.FloatTensor(5, 5).uniform_() > 0.5).int(),\n",
    "        W_int: torch.Tensor = None,\n",
    "        log_h_int: torch.Tensor = None,\n",
    "        global_time: bool = False,\n",
    "    ):\n",
    "        device = W.device\n",
    "        super().__init__()\n",
    "        self.n_int = n_int\n",
    "        if global_time:\n",
    "            self.log_h = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.log_phi = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.tau = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.o = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "        else:\n",
    "            self.log_h = torch.nn.Parameter(log_h_int.repeat(W.shape[0],1)*W)\n",
    "            self.log_phi = torch.nn.Parameter(torch.ones(W.shape).to(device)*W)\n",
    "            self.tau = torch.nn.Parameter(torch.ones(W.shape).to(device)*W*10)\n",
    "            self.o = torch.nn.Parameter(torch.ones(W.shape).to(device)*W)\n",
    "\n",
    "        self.mask_m = W\n",
    "        self.global_time = global_time\n",
    "\n",
    "        ## initialize grn\n",
    "        self.grn = torch.nn.Parameter(W_int*self.mask_m)\n",
    "        \n",
    "        ## initilize gamma and beta\n",
    "        self.beta_mean_unconstr = torch.nn.Parameter(0.5 * torch.ones(n_int))\n",
    "        self.gamma_mean_unconstr = torch.nn.Parameter(-1 * torch.ones(n_int))\n",
    "        self.alpha_unconstr_bias = torch.nn.Parameter(torch.zeros(n_int))\n",
    "        self.alpha_unconstr_max = torch.nn.Parameter(torch.randn(n_int))\n",
    "        # calculating emulating matrix\n",
    "    \n",
    "    ### define hook to froze the parameters\n",
    "    def _set_mask_grad(self):\n",
    "        self.hooks_grn = []\n",
    "        if not self.global_time:\n",
    "            self.hooks_log_h = []\n",
    "            self.hooks_log_phi = []\n",
    "            self.hooks_tau = []\n",
    "            self.hooks_o = []\n",
    "        #mask_m = self.mask_m\n",
    "        \n",
    "        def _hook_mask_no_regulator(grad):\n",
    "            return grad * self.mask_m\n",
    "\n",
    "        w_grn = self.grn.register_hook(_hook_mask_no_regulator)\n",
    "        self.hooks_grn.append(w_grn)\n",
    "        if not self.global_time:\n",
    "            w_log_h = self.log_h.register_hook(_hook_mask_no_regulator)\n",
    "            w_log_phi = self.log_phi.register_hook(_hook_mask_no_regulator)\n",
    "            w_tau = self.tau.register_hook(_hook_mask_no_regulator)\n",
    "            w_o = self.o.register_hook(_hook_mask_no_regulator)\n",
    "\n",
    "            self.hooks_log_h.append(w_log_h)\n",
    "            self.hooks_log_phi.append(w_log_phi)\n",
    "            self.hooks_tau.append(w_tau)\n",
    "            self.hooks_o.append(w_o)\n",
    "\n",
    "    def emulator(t,log_h_v,log_phi_v,tau_v,o_v):\n",
    "        pre = torch.exp(log_h_v)*torch.exp(-torch.exp(phi_v)*(t-tau_v)**2)+o_v\n",
    "        \n",
    "        return pre\n",
    "\n",
    "    def emulation_all(self,t: torch.Tensor = None):\n",
    "        if self.global_time:\n",
    "            # broadcasting the time t\n",
    "            t = t.repeat((self.mask_m.shape[0],1))\n",
    "\n",
    "        emulate_m = torch.zeros([self.mask_m.shape[0], self.mask_m.shape[1], t.shape[1]])\n",
    "\n",
    "        h = torch.exp(self.log_h)\n",
    "        phi = torch.exp(self.log_phi)\n",
    "        for i in range(t.shape[1]):\n",
    "            # for each time stamps, predict the emulator predict value\n",
    "            tt = t[:,i]\n",
    "            emu = h * torch.exp(-phi*(tt.reshape((len(tt),1))-self.tau)**2) + self.o\n",
    "            emulate_m[:,:,i] = emu\n",
    "\n",
    "        return emulate_m\n",
    "\n",
    "\n",
    "    def forward(self,t, y):\n",
    "        ## split x into unspliced and spliced readout\n",
    "        ## x is a matrix with (G*2), in which row is a subgraph (batch)\n",
    "        #print(y)\n",
    "        #assert torch.jit.isinstance(args, torch.Tensor)\n",
    "        #assert torch.jit.isinstance(t, torch.Tensor)\n",
    "        #assert torch.jit.isinstance(y, torch.Tensor)\n",
    "        \n",
    "        args = torch.arange(0, self.n_int).reshape((self.n_int,1))\n",
    "        dim = args.shape[0]\n",
    "        locate = args.ravel()\n",
    "        if len(y.shape) == 1:\n",
    "            u = y[0]\n",
    "            s = y[1]\n",
    "        else:  \n",
    "            u = y[:,0]\n",
    "            s = y[:,1]\n",
    "\n",
    "        if self.global_time:\n",
    "            u = u[locate]\n",
    "            s = s[locate]\n",
    "            ## when use global time, t is a single value\n",
    "            T = t.repeat((dim,1))\n",
    "\n",
    "            ## calculate emulator vector\n",
    "            h = torch.exp(self.log_h)\n",
    "            phi = torch.exp(self.log_phi)\n",
    "            emu = h[locate,:] * torch.exp(-phi[locate,:]*(T.reshape((dim,1))-self.tau[locate,:])**2) + self.o[locate,:]\n",
    "            \n",
    "            ## Use the Emulator matrix to predict alpha\n",
    "            emu = emu * self.grn[locate,:]\n",
    "            alpha_unconstr = emu.sum(dim=1)\n",
    "            alpha_unconstr = alpha_unconstr + self.alpha_unconstr[locate]\n",
    "            \n",
    "            ## Generate kinetic rate\n",
    "            beta = torch.clamp(F.softplus(self.beta_mean_unconstr[locate]), 0, 50)\n",
    "            gamma = torch.clamp(F.softplus(self.gamma_mean_unconstr[locate]), 0, 50)\n",
    "            alpha = torch.clamp(F.softplus(alpha_unconstr),0,50)\n",
    "\n",
    "            ## Predict velocity\n",
    "            du = alpha - beta*u\n",
    "            ds = beta*u - gamma*s\n",
    "\n",
    "            du = du.reshape((dim,1))\n",
    "            ds = ds.reshape((dim,1))\n",
    "\n",
    "            v = torch.concatenate([du,ds],axis = 1)\n",
    "\n",
    "        else:\n",
    "            ## calculate emulator value\n",
    "            ## t is a vector per gene (G*1)\n",
    "            ## extract the corresponding gene\n",
    "            #u = u[locate]\n",
    "            #s = s[locate]\n",
    "            #T = t[locate]\n",
    "\n",
    "            ## Build Emulator matrix\n",
    "            h = torch.exp(self.log_h)\n",
    "            phi = torch.exp(self.log_phi)\n",
    "            #emu = h[locate,:] * torch.exp(-phi[locate,:]*(T.reshape((dim,1))-self.tau[locate,:])**2) + self.o[locate,:]\n",
    "            emu = h * torch.exp(-phi*(t.reshape((dim,1)) - self.tau)**2) + self.o\n",
    "\n",
    "            ## Use the Emulator matrix to predict alpha\n",
    "            #emu = emu * self.grn[locate,:]\n",
    "            emu = emu * self.grn\n",
    "            \n",
    "            alpha_unconstr = emu.sum(dim=1)\n",
    "            #alpha_unconstr = alpha_unconstr + self.alpha_unconstr_bias[locate]\n",
    "            alpha_unconstr = alpha_unconstr + self.alpha_unconstr_bias\n",
    "\n",
    "            ## Generate kinetic rate\n",
    "            #beta = torch.clamp(F.softplus(self.beta_mean_unconstr[locate]), 0, 50)\n",
    "            #gamma = torch.clamp(F.softplus(self.gamma_mean_unconstr[locate]), 0, 50)\n",
    "            beta = torch.clamp(F.softplus(self.beta_mean_unconstr), 0, 50)\n",
    "            gamma = torch.clamp(F.softplus(self.gamma_mean_unconstr), 0, 50)\n",
    "            alpha = torch.clamp(alpha_unconstr,0,)\n",
    "            alpha = F.softsign(alpha)*torch.clamp(F.softplus(self.alpha_unconstr_max), 0, 50)\n",
    "\n",
    "            ## Predict velocity\n",
    "            du = alpha - beta*u\n",
    "            ds = beta*u - gamma*s\n",
    "            \n",
    "            du = du.reshape((dim,1))\n",
    "            ds = ds.reshape((dim,1))\n",
    "\n",
    "            v = torch.concatenate([du,ds],axis = 1)\n",
    "\n",
    "        if len(y.shape) == 1:\n",
    "            v = v.view(-1)\n",
    "\n",
    "        return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fe2358c4-1a9b-4433-bff2-c4c7388078ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_encoder = velocity_encoder(n_int = rgv_m.model.n_targets, alpha_unconstr_init = rgv_m.model.f_t.alpha_unconstr_bias,log_h_int = rgv_m.model.f_t.log_h[0,:],\n",
    "                    W = W.T, W_int = rgv_m.model.f_t.grn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b484710e-126a-4718-8daa-63d498e86fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = t.T\n",
    "t_eval, index_matrix = torch.sort(T, dim=1)\n",
    "\n",
    "index2 = (t_eval[:,:-1] != t_eval[:,1:])\n",
    "## estimate the shift value\n",
    "subtraction_values,_ = torch.where((t_eval[:,1:] - t_eval[:,:-1])>0, (t_eval[:,1:] - t_eval[:,:-1]), torch.inf).min(axis=1)\n",
    "## replace inf value = 0\n",
    "subtraction_values[subtraction_values == float(\"Inf\")] = 0\n",
    "\n",
    "true_tensor = torch.ones((t_eval.shape[0],1), dtype=torch.bool)\n",
    "index2 = torch.cat((index2, true_tensor.to(index2.device)),dim=1) ## index2 is used to get unique time points as odeint requires strictly increasing/decreasing time points\n",
    "\n",
    "subtraction_values = subtraction_values[None, :].repeat(index2.shape[1], 1).T\n",
    "t_eval[index2 == False] -= subtraction_values[index2 == False]*0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "649cc9b2-e6ea-4fcc-bb39-bbbf4e7823fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.1117,  2.8942,  3.4313,  ..., 16.2034, 16.9352, 17.8356],\n",
       "        [ 1.4085,  2.9288,  3.3572,  ..., 14.9389, 15.8927, 16.6538],\n",
       "        [ 2.4939,  2.7330,  4.3067,  ..., 15.0287, 16.4146, 17.4256],\n",
       "        ...,\n",
       "        [ 1.1464,  3.7646,  5.2279,  ..., 16.8554, 16.9816, 17.1428],\n",
       "        [ 1.9435,  4.4762,  4.6579,  ..., 16.6903, 17.2010, 17.5247],\n",
       "        [ 1.7885,  2.8443,  3.9657,  ..., 14.3889, 14.9995, 16.9701]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "531cdadb-a74d-47b5-be3c-50d9f96ff23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.zeros([250,2])\n",
    "indices = torch.arange(0, x0.shape[0]).reshape((x0.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4fc0b172-0bbb-4cfb-86c8-f38029a7ab8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.399154\n"
     ]
    }
   ],
   "source": [
    "import torchode as to\n",
    "import datetime\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "\n",
    "T = t.T\n",
    "indices = torch.argsort(T, dim=1)\n",
    "t_eval = torch.gather(T, dim = 1, index = indices)\n",
    "\n",
    "index2 = (t_eval[:,:-1] != t_eval[:,1:])\n",
    "## estimate the shift value\n",
    "subtraction_values,_ = torch.where((t_eval[:,1:] - t_eval[:,:-1])>0, (t_eval[:,1:] - t_eval[:,:-1]), torch.inf).min(axis=1)\n",
    "## replace inf value = 0\n",
    "subtraction_values[subtraction_values == float(\"Inf\")] = 0\n",
    "\n",
    "true_tensor = torch.ones((t_eval.shape[0],1), dtype=torch.bool)\n",
    "index2 = torch.cat((index2, true_tensor.to(index2.device)),dim=1) ## index2 is used to get unique time points as odeint requires strictly increasing/decreasing time points\n",
    "\n",
    "subtraction_values = subtraction_values[None, :].repeat(index2.shape[1], 1).T\n",
    "t_eval[index2 == False] -= subtraction_values[index2 == False]*0.1\n",
    "\n",
    "term = to.ODETerm(v_encoder)\n",
    "step_method = to.Dopri5(term=term)\n",
    "step_size_controller = to.IntegralController(atol=1e-9, rtol=1e-6, term=term)\n",
    "#step_size_controller = to.FixedStepController()\n",
    "solver = to.AutoDiffAdjoint(step_method, step_size_controller)\n",
    "#jit_solver = torch.jit.script(solver)\n",
    "dt0 = torch.full((x0.shape[0],), 1)\n",
    "sol = solver.solve(to.InitialValueProblem(y0=x0, t_eval=t_eval))\n",
    "\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d990d10f-7d8c-4551-8966-5c5523505dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.0304, 12.7919, 10.4655,  ..., 12.9181, 13.4688,  6.1694],\n",
       "        [ 4.3107, 14.8696,  2.9288,  ..., 16.6538, 13.3592,  5.6252],\n",
       "        [ 2.7330, 17.4256,  6.4632,  ..., 11.3390,  5.5270,  6.9934],\n",
       "        ...,\n",
       "        [ 7.8638,  6.2608, 11.8545,  ..., 10.7559, 16.3847, 16.8554],\n",
       "        [ 8.7109, 16.5141,  6.6571,  ..., 13.8724, 15.6707, 10.7586],\n",
       "        [ 2.8443, 12.8971, 10.0709,  ...,  8.4398,  7.6313, 11.7996]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "021086fb-0c1e-4775-b233-b2a202ac292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_u = torch.zeros_like(t_eval)\n",
    "for i in range(t_eval.size(0)):\n",
    "    pre_u[i][indices[i]] = sol.ys[:,:,0][i]\n",
    "    \n",
    "pre_s = torch.zeros_like(t_eval)\n",
    "for i in range(t_eval.size(0)):\n",
    "    pre_s[i][indices[i]] = sol.ys[:,:,1][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "abf027f5-7de8-4c6a-b6bf-b0fb4dc264b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6583, 0.7938, 0.9838,  ..., 0.7788, 0.7310, 0.6510],\n",
       "        [0.5976, 0.6515, 0.4908,  ..., 0.6381, 0.7063, 0.6248],\n",
       "        [0.1320, 0.6367, 0.6219,  ..., 1.0028, 0.6021, 0.6273],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0757, 0.0758, 0.0749,  ..., 0.0775, 0.0760, 0.1088],\n",
       "        [0.0755, 0.1242, 0.1583,  ..., 0.1174, 0.1172, 0.1367]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cfa89c2-5365-4975-9a16-9e741c5e4749",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4204, 2.5681, 2.2019,  ..., 2.5633, 2.5277, 1.2293],\n",
       "        [0.8577, 2.3252, 0.3798,  ..., 2.1849, 2.4709, 1.2136],\n",
       "        [0.0156, 2.1399, 1.1551,  ..., 2.3432, 0.8982, 1.2757],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.1939, 0.2451, 0.1573,  ..., 0.2554, 0.2478, 0.2422],\n",
       "        [0.0404, 0.4000, 0.3402,  ..., 0.2988, 0.2798, 0.3995]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ffa1c382-c1e8-4308-92fb-79c72c3291c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7287, 1.2348, 0.8037,  ..., 0.0000, 0.2128, 0.1554],\n",
       "        [2.6200, 2.3405, 2.1547,  ..., 0.0000, 0.2467, 0.4071],\n",
       "        [2.3092, 0.8660, 1.5925,  ..., 0.0000, 0.1924, 0.3574],\n",
       "        ...,\n",
       "        [2.6132, 2.1936, 2.4419,  ..., 0.0000, 0.2592, 0.3274],\n",
       "        [2.5697, 2.4955, 1.4641,  ..., 0.0000, 0.2499, 0.3164],\n",
       "        [1.6260, 1.4793, 1.6507,  ..., 0.0000, 0.2522, 0.4095]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "40bb11fc-cd44-4677-9114-afd46acd55eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 250])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "19c01ce7-6369-4279-bad1-cacc0c070cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 250])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b7e774ff-486d-4ca7-9fe1-c1d89158a896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "corr = scipy.stats.spearmanr(u.cpu().detach().numpy(), u2.detach().numpy())\n",
    "corr = np.diagonal(corr.statistic[0:250,250:(250*2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "937f5f70-3800-4b6a-a516-3698cae274aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99555061, 0.99955506, 0.99822024, 0.96440489, 0.91724138,\n",
       "       0.78598443,        nan, 0.88565072, 0.99510567, 0.98353726,\n",
       "       0.9105673 , 0.8407119 , 0.99822024, 0.89010011, 0.94660734,\n",
       "       0.97285873, 0.79755284, 0.99866518,        nan,        nan,\n",
       "              nan, 0.9919911 , 0.99911012, 0.90344828,        nan,\n",
       "       0.98531702, 0.78998888, 0.97196885, 0.95951057, 0.92791991,\n",
       "       0.86918799, 0.89187987, 0.99377086, 0.80155729, 0.96751947,\n",
       "       0.95372636, 0.91679644, 0.95951057, 0.91457175, 0.90567297,\n",
       "       0.85539488, 0.9919911 , 0.86073415, 0.97063404, 0.98175751,\n",
       "       0.98042269, 0.9839822 , 0.91190211, 0.74549499, 0.90656285,\n",
       "       0.92614016,        nan, 0.90478309, 0.9025584 , 0.90300334,\n",
       "              nan, 0.7432703 , 0.96307008, 0.94126808, 0.96974416,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "       0.96395996, 0.90211346, 0.74060067,        nan, 0.98665184,\n",
       "       0.99777531, 0.96751947, 0.7374861 , 0.96929922, 0.97953281,\n",
       "       0.9942158 , 0.99154616, 0.79399333, 0.94794216, 0.95061179,\n",
       "       0.97285873,        nan, 0.91145717, 0.9025584 , 0.98932147,\n",
       "       0.92525028, 0.85317019, 0.83581758, 0.74682981,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan, 0.99822024,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "       0.92213571, 0.75483871, 0.97953281, 0.96395996, 0.88209121,\n",
       "       0.90878754,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan, 0.96351502,\n",
       "       0.96129032, 0.98309232, 0.96307008, 0.99822024, 0.99955506,\n",
       "       0.98309232, 1.        , 1.        , 0.98887653, 0.98887653,\n",
       "       1.        , 1.        ,        nan,        nan, 0.97196885,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan,        nan,        nan, 0.99688543, 0.99688543,\n",
       "              nan, 0.89098999,        nan,        nan,        nan,\n",
       "       0.99955506,        nan, 0.98264739,        nan,        nan,\n",
       "              nan, 0.99377086,        nan,        nan, 0.96840934,\n",
       "              nan, 0.99644049,        nan,        nan,        nan,\n",
       "              nan, 0.9919911 ,        nan,        nan,        nan,\n",
       "       0.98887653,        nan,        nan,        nan, 0.99822024,\n",
       "              nan, 0.98843159,        nan, 0.99777531,        nan,\n",
       "       0.98709677, 0.99822024, 0.96351502,        nan, 0.98531702,\n",
       "              nan,        nan, 0.99243604, 0.94883204,        nan,\n",
       "              nan,        nan, 0.97997775,        nan,        nan,\n",
       "              nan,        nan,        nan, 0.9942158 , 0.95105673,\n",
       "       0.96529477,        nan, 0.96751947,        nan,        nan,\n",
       "       0.98220245,        nan, 0.96929922, 0.96040044,        nan,\n",
       "              nan, 0.99733037,        nan, 0.96929922, 0.99466073,\n",
       "       0.95061179,        nan,        nan, 0.9942158 ,        nan,\n",
       "              nan,        nan,        nan,        nan,        nan,\n",
       "              nan, 0.99065628,        nan, 0.97641824, 0.98976641])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "809c90f1-09ab-457e-9eed-cb7f53663214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tensor:\n",
      "tensor([[10.1061, 10.0764, 12.3378,  ...,  6.5726,  5.6128, 13.1180],\n",
      "        [13.0245,  9.9245, 10.4516,  ...,  7.7540,  9.4624, 13.4341],\n",
      "        [ 7.4929, 10.2253,  4.3061,  ..., 14.6500,  4.2954,  4.8138],\n",
      "        ...,\n",
      "        [11.2416,  5.1236, 12.6985,  ..., 15.4801, 14.9228,  9.4909],\n",
      "        [ 4.1051,  3.6933, 14.1141,  ...,  7.9020,  9.5526, 11.3433],\n",
      "        [11.1905, 14.4320, 10.2723,  ...,  6.4391, 10.6447,  8.6281]])\n",
      "Sorted tensor:\n",
      "tensor([[ 1.2378,  3.1580,  3.3272,  ..., 16.2686, 16.5700, 16.9063],\n",
      "        [ 1.3134,  1.9177,  3.2865,  ..., 17.4862, 17.6019, 18.3128],\n",
      "        [ 0.5776,  3.6837,  4.2311,  ..., 15.2332, 16.0353, 17.9717],\n",
      "        ...,\n",
      "        [ 3.3419,  3.3658,  3.4432,  ..., 16.9976, 17.3424, 19.3139],\n",
      "        [ 1.9450,  3.6933,  4.1051,  ..., 15.2883, 15.3973, 17.3052],\n",
      "        [ 2.1951,  3.8047,  5.3273,  ..., 16.3272, 16.5309, 18.4443]])\n",
      "Unsorted tensor:\n",
      "tensor([[10.1061, 10.0764, 12.3378,  ...,  6.5726,  5.6128, 13.1180],\n",
      "        [13.0245,  9.9245, 10.4516,  ...,  7.7540,  9.4624, 13.4341],\n",
      "        [ 7.4929, 10.2253,  4.3061,  ..., 14.6500,  4.2954,  4.8138],\n",
      "        ...,\n",
      "        [11.2416,  5.1236, 12.6985,  ..., 15.4801, 14.9228,  9.4909],\n",
      "        [ 4.1051,  3.6933, 14.1141,  ...,  7.9020,  9.5526, 11.3433],\n",
      "        [11.1905, 14.4320, 10.2723,  ...,  6.4391, 10.6447,  8.6281]])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([[3, 1, 4],\n",
    "                       [5, 2, 1],\n",
    "                       [0, 2, 6]])\n",
    "\n",
    "# Create an index matrix that sorts each row of the tensor\n",
    "sorted_indices = torch.argsort(T, dim=1)\n",
    "\n",
    "# Use the sorted index matrix to sort each row of the tensor\n",
    "sorted_tensor = torch.gather(T, 1, sorted_indices)\n",
    "\n",
    "# Use the sorted index matrix to transform the sorted tensor back into the unsorted tensor\n",
    "unsorted_tensor = torch.zeros_like(sorted_tensor)\n",
    "for i in range(sorted_indices.size(0)):\n",
    "    unsorted_tensor[i][sorted_indices[i]] = sorted_tensor[i]\n",
    "\n",
    "# Print the original, sorted, and unsorted tensors\n",
    "print(\"Original tensor:\")\n",
    "print(T)\n",
    "print(\"Sorted tensor:\")\n",
    "print(sorted_tensor)\n",
    "print(\"Unsorted tensor:\")\n",
    "print(unsorted_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139d9a29-623b-4cbd-8425-71f23d61877f",
   "metadata": {},
   "source": [
    "# test raw version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "67c6d822-bac7-4553-b3a9-6acb7cb2f1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class alpha_encoder(nn.Module):\n",
    "    \"\"\" \n",
    "    encode the time dependent alpha (f)\n",
    "    time dependent transcription rate is determined by upstream emulator\n",
    "\n",
    "    \"\"\"                 \n",
    "    def __init__(\n",
    "        self,\n",
    "        n_int: int = 5,\n",
    "        alpha_unconstr_init: torch.Tensor = None,\n",
    "        W: torch.Tensor = (torch.FloatTensor(5, 5).uniform_() > 0.5).int(),\n",
    "        W_int: torch.Tensor = None,\n",
    "        log_h_int: torch.Tensor = None,\n",
    "        global_time: bool = False,\n",
    "    ):\n",
    "        device = W.device\n",
    "        super().__init__()\n",
    "        self.n_int = n_int\n",
    "        if global_time:\n",
    "            self.log_h = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.log_phi = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.tau = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "            self.o = torch.nn.Parameter(torch.randn(W.shape[1]))\n",
    "        else:\n",
    "            self.log_h = torch.nn.Parameter(log_h_int.repeat(W.shape[0],1)*W)\n",
    "            self.log_phi = torch.nn.Parameter(torch.ones(W.shape).to(device)*W)\n",
    "            self.tau = torch.nn.Parameter(torch.ones(W.shape).to(device)*W*10)\n",
    "            self.o = torch.nn.Parameter(torch.ones(W.shape).to(device)*W)\n",
    "\n",
    "        self.mask_m = W\n",
    "        self.global_time = global_time\n",
    "\n",
    "        ## initialize grn\n",
    "        self.grn = torch.nn.Parameter(W_int*self.mask_m)\n",
    "        \n",
    "        ## initilize gamma and beta\n",
    "        self.beta_mean_unconstr = torch.nn.Parameter(0.5 * torch.ones(n_int))\n",
    "        self.gamma_mean_unconstr = torch.nn.Parameter(-1 * torch.ones(n_int))\n",
    "        self.alpha_unconstr_bias = torch.nn.Parameter(torch.zeros(n_int))\n",
    "        self.alpha_unconstr_max = torch.nn.Parameter(torch.randn(n_int))\n",
    "        # calculating emulating matrix\n",
    "    \n",
    "    ### define hook to froze the parameters\n",
    "    def _set_mask_grad(self):\n",
    "        self.hooks_grn = []\n",
    "        if not self.global_time:\n",
    "            self.hooks_log_h = []\n",
    "            self.hooks_log_phi = []\n",
    "            self.hooks_tau = []\n",
    "            self.hooks_o = []\n",
    "        #mask_m = self.mask_m\n",
    "        \n",
    "        def _hook_mask_no_regulator(grad):\n",
    "            return grad * self.mask_m\n",
    "\n",
    "        w_grn = self.grn.register_hook(_hook_mask_no_regulator)\n",
    "        self.hooks_grn.append(w_grn)\n",
    "        if not self.global_time:\n",
    "            w_log_h = self.log_h.register_hook(_hook_mask_no_regulator)\n",
    "            w_log_phi = self.log_phi.register_hook(_hook_mask_no_regulator)\n",
    "            w_tau = self.tau.register_hook(_hook_mask_no_regulator)\n",
    "            w_o = self.o.register_hook(_hook_mask_no_regulator)\n",
    "\n",
    "            self.hooks_log_h.append(w_log_h)\n",
    "            self.hooks_log_phi.append(w_log_phi)\n",
    "            self.hooks_tau.append(w_tau)\n",
    "            self.hooks_o.append(w_o)\n",
    "\n",
    "    def emulator(t,log_h_v,log_phi_v,tau_v,o_v):\n",
    "        pre = torch.exp(log_h_v)*torch.exp(-torch.exp(phi_v)*(t-tau_v)**2)+o_v\n",
    "        \n",
    "        return pre\n",
    "\n",
    "    def emulation_all(self,t: torch.Tensor = None):\n",
    "        if self.global_time:\n",
    "            # broadcasting the time t\n",
    "            t = t.repeat((self.mask_m.shape[0],1))\n",
    "\n",
    "        emulate_m = torch.zeros([self.mask_m.shape[0], self.mask_m.shape[1], t.shape[1]])\n",
    "\n",
    "        h = torch.exp(self.log_h)\n",
    "        phi = torch.exp(self.log_phi)\n",
    "        for i in range(t.shape[1]):\n",
    "            # for each time stamps, predict the emulator predict value\n",
    "            tt = t[:,i]\n",
    "            emu = h * torch.exp(-phi*(tt.reshape((len(tt),1))-self.tau)**2) + self.o\n",
    "            emulate_m[:,:,i] = emu\n",
    "\n",
    "        return emulate_m\n",
    "\n",
    "\n",
    "    def forward(self,t,g):\n",
    "        ## Encode \n",
    "\n",
    "        if self.global_time:\n",
    "            u = u[locate]\n",
    "            s = s[locate]\n",
    "            ## when use global time, t is a single value\n",
    "            T = t.repeat((dim,1))\n",
    "\n",
    "            ## calculate emulator vector\n",
    "            h = torch.exp(self.log_h)\n",
    "            phi = torch.exp(self.log_phi)\n",
    "            emu = h[locate,:] * torch.exp(-phi[locate,:]*(T.reshape((dim,1))-self.tau[locate,:])**2) + self.o[locate,:]\n",
    "            \n",
    "            ## Use the Emulator matrix to predict alpha\n",
    "            emu = emu * self.grn[locate,:]\n",
    "            alpha_unconstr = emu.sum(dim=1)\n",
    "            alpha_unconstr = alpha_unconstr + self.alpha_unconstr[locate]\n",
    "            \n",
    "            ## Generate kinetic rate\n",
    "            beta = torch.clamp(F.softplus(self.beta_mean_unconstr[locate]), 0, 50)\n",
    "            gamma = torch.clamp(F.softplus(self.gamma_mean_unconstr[locate]), 0, 50)\n",
    "            alpha = torch.clamp(F.softplus(alpha_unconstr),0,50)\n",
    "\n",
    "            ## Predict velocity\n",
    "            du = alpha - beta*u\n",
    "            ds = beta*u - gamma*s\n",
    "\n",
    "            du = du.reshape((dim,1))\n",
    "            ds = ds.reshape((dim,1))\n",
    "\n",
    "            v = torch.concatenate([du,ds],axis = 1)\n",
    "\n",
    "        else:\n",
    "            ## calculate emulator value\n",
    "            ## output the f_g(t)\n",
    "            \n",
    "            ## Build Emulator matrix for gene g\n",
    "            h = torch.exp(self.log_h)[g,:].view(-1)\n",
    "            phi = torch.exp(self.log_phi)[g,:].view(-1)\n",
    "            tau = self.tau[g,:].view(-1)\n",
    "            o = self.o[g,:].view(-1)\n",
    "            w = self.grn[g,:].view(-1)\n",
    "            bias = self.alpha_unconstr_bias[g]\n",
    "\n",
    "            #emu = h[locate,:] * torch.exp(-phi[locate,:]*(T.reshape((dim,1))-self.tau[locate,:])**2) + self.o[locate,:]\n",
    "            if len(t.shape) > 1:\n",
    "                alpha_all = []\n",
    "                for i in range(t.shape[0]):\n",
    "                    t_x = t[i]\n",
    "                    emu = h * torch.exp(-phi*(t_x - tau)**2) + o\n",
    "\n",
    "                    ## Use the Emulator matrix to predict alpha\n",
    "                    #emu = emu * self.grn[locate,:]\n",
    "                    emu = emu * w\n",
    "\n",
    "                    alpha_unconstr = emu.sum()\n",
    "                    #alpha_unconstr = alpha_unconstr + self.alpha_unconstr_bias[locate]\n",
    "                    alpha_unconstr = alpha_unconstr + bias\n",
    "\n",
    "                    ## Generate transcription kinetic rate for time t\n",
    "                    alpha = torch.clamp(alpha_unconstr,0,)\n",
    "                    alpha = F.softsign(alpha)\n",
    "                    alpha_all.append(alpha)\n",
    "                    \n",
    "                alpha = torch.tensor(alpha_all)\n",
    "            else:\n",
    "                emu = h * torch.exp(-phi*(t - tau)**2) + o\n",
    "\n",
    "                ## Use the Emulator matrix to predict alpha\n",
    "                #emu = emu * self.grn[locate,:]\n",
    "                emu = emu * w\n",
    "\n",
    "                alpha_unconstr = emu.sum()\n",
    "                #alpha_unconstr = alpha_unconstr + self.alpha_unconstr_bias[locate]\n",
    "                alpha_unconstr = alpha_unconstr + bias\n",
    "\n",
    "                ## Generate transcription kinetic rate for time t\n",
    "                alpha = torch.clamp(alpha_unconstr,0,)\n",
    "                alpha = F.softsign(alpha)\n",
    "\n",
    "        return alpha\n",
    "    \n",
    "def SolveInitialValueProblem(f_t, x0, t0, t_eval):\n",
    "    \n",
    "    ## generate the prediction of unspliced/spliced readout at time t for every gene\n",
    "    ## use torchquad integral, different with torchode, the t_eval no longer need to be ordered\n",
    "    \n",
    "    ## get the kinetic parameters\n",
    "    beta = torch.clamp(F.softplus(f_t.beta_mean_unconstr), 0, 50)\n",
    "    gamma = torch.clamp(F.softplus(f_t.gamma_mean_unconstr), 0, 50)\n",
    "    alpha_max = torch.clamp(F.softplus(f_t.alpha_unconstr_max),0,50)\n",
    "\n",
    "    ## define integral function\n",
    "    def integral_alpha_beta(f_t, tt, t0, beta, g):\n",
    "        f_i = lambda t: f_t(t,g)*torch.exp(beta[g]*t.view(-1))\n",
    "        integration_domain = [[t0[g],tt]]\n",
    "        result = simp.integrate(f_i, dim=1, N=101, integration_domain=integration_domain)\n",
    "        return result\n",
    "        \n",
    "    def integral_alpha_gamma(f_t, tt,t0, gamma, g):\n",
    "        f_i = lambda t: f_t(t,g)*torch.exp(gamma[g]*t.view(-1))\n",
    "        integration_domain = [[t0[g],tt]]\n",
    "        result = simp.integrate(f_i, dim=1, N=101, integration_domain=integration_domain)\n",
    "        return result\n",
    "\n",
    "    ## get the initial condition (i.e. u,s = 0,0)\n",
    "    u0 = x0[:,0].view(-1)\n",
    "    s0 = x0[:,1].view(-1)\n",
    "    pre_u = torch.zeros(t_eval.shape)\n",
    "    pre_s = torch.zeros(t_eval.shape)\n",
    "    \n",
    "    ## build for loop to generate readout for each targets\n",
    "    for g, t in enumerate(t_eval):\n",
    "        u0g = u0[g]\n",
    "        s0g = s0[g]\n",
    "        \n",
    "        ## calculate integral for gene g\n",
    "        integral_tensor_alpha_beta = torch.tensor(list(map(lambda tt: integral_alpha_beta(tt=tt,f_t=f_t,t0=t0,beta = beta,g = g), t)))\n",
    "        integral_tensor_alpha_gamma = torch.tensor(list(map(lambda tt: integral_alpha_gamma(tt=tt,f_t=f_t,t0=t0,gamma = gamma,g = g), t)))\n",
    "        \n",
    "        #ug = u0g*torch.exp(-beta[g]*t) + alpha_max[g]*torch.exp(-beta[g]*t)*integral_tensor_alpha_beta\n",
    "        #sg = s0g*torch.exp(-gamma[g]*t) + \\\n",
    "        #    ( (alpha_max[g]*beta[g])/(gamma[g] - beta[g]) )*(torch.exp(-beta[g]*t)*integral_tensor_alpha_beta - torch.exp(-gamma[g]*t)*integral_tensor_alpha_gamma) + \\\n",
    "        #    ( (beta[g]*u0g)/(gamma[g] - beta[g]) )*(torch.exp(-beta[g]*t) - torch.exp(-gamma[g]*t))\n",
    "        \n",
    "        pre_u[g,:] = integral_tensor_alpha_beta\n",
    "        pre_s[g,:] = integral_tensor_alpha_gamma\n",
    "        #print(g)\n",
    "    return pre_u, pre_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf362a40-e524-4615-ad87-fa6c42fbe77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_t = alpha_encoder(n_int = rgv_m.model.n_targets, alpha_unconstr_init = rgv_m.model.v_encoder.alpha_unconstr_bias,log_h_int = rgv_m.model.v_encoder.log_h[0,:],\n",
    "                    W = W.T, W_int = rgv_m.model.v_encoder.grn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "20d93aa2-dd0c-4ecb-904a-b098a65ed102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7.0304,  4.3107,  2.7330,  ...,  7.8638,  8.7109,  2.8443],\n",
       "        [12.7919, 14.8696, 17.4256,  ...,  6.2608, 16.5141, 12.8971],\n",
       "        [10.4655,  2.9288,  6.4632,  ..., 11.8545,  6.6571, 10.0709],\n",
       "        ...,\n",
       "        [11.4710, 10.4500,  6.0351,  ..., 11.6317,  9.8647, 12.9834],\n",
       "        [ 4.1471,  6.8124, 16.2744,  ...,  7.4569,  6.7320,  7.3953],\n",
       "        [11.0352, 13.1345,  5.0188,  ..., 13.9301,  6.2952,  9.8744]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "280c14ce-a910-4078-b625-95bd22949d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6466, 0.6188, 0.6188, 0.5963, 0.6424, 0.7336, 0.0000, 0.2757, 0.2469,\n",
      "        0.2423, 0.7792, 0.6513, 0.6605, 0.7462, 0.6503, 0.6628, 0.7865, 0.6536,\n",
      "        0.0000, 0.0000, 0.0000, 0.1511, 0.1516, 0.6611, 0.0000, 0.5174, 0.5219,\n",
      "        0.2701, 0.3650, 0.5344, 0.2397, 0.6621, 0.6678, 0.7116, 0.4560, 0.4577,\n",
      "        0.4551, 0.4865, 0.6590, 0.6602, 0.6613, 0.5605, 0.5485, 0.4566, 0.5268,\n",
      "        0.4721, 0.5381, 0.6570, 0.5523, 0.4723, 0.5593, 0.0000, 0.6158, 0.5800,\n",
      "        0.5940, 0.0000, 0.6470, 0.3482, 0.3598, 0.5011, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.7044, 0.6917, 0.4901, 0.0000, 0.5312, 0.5418, 0.5365,\n",
      "        0.5241, 0.6061, 0.6151, 0.5175, 0.6175, 0.3918, 0.6821, 0.5146, 0.5236,\n",
      "        0.0000, 0.6974, 0.7034, 0.6863, 0.6519, 0.6617, 0.7456, 0.6605, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.4037, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6621, 0.6624, 0.6701,\n",
      "        0.6624, 0.6589, 0.6614, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6497,\n",
      "        0.6554, 0.6986, 0.6533, 0.7904, 0.7934, 0.8360, 0.7903, 0.7863, 0.7869,\n",
      "        0.7873, 0.7856, 0.7884, 0.0000, 0.0000, 0.1086, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.1193, 0.0209, 0.0000, 0.0596, 0.0000, 0.0000, 0.0000, 0.1123,\n",
      "        0.0000, 0.0131, 0.0000, 0.0000, 0.0000, 0.1695, 0.0000, 0.0000, 0.1208,\n",
      "        0.0000, 0.0218, 0.0000, 0.0000, 0.0000, 0.0000, 0.0099, 0.0000, 0.0000,\n",
      "        0.0000, 0.0862, 0.0000, 0.0000, 0.0000, 0.0380, 0.0000, 0.1168, 0.0000,\n",
      "        0.0363, 0.0000, 0.0990, 0.0299, 0.0744, 0.0000, 0.1346, 0.0000, 0.0000,\n",
      "        0.0644, 0.0819, 0.0000, 0.0000, 0.0000, 0.1223, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0868, 0.2414, 0.1009, 0.0000, 0.0752, 0.0000, 0.0000,\n",
      "        0.0366, 0.0000, 0.1264, 0.1191, 0.0000, 0.0000, 0.0776, 0.0000, 0.1700,\n",
      "        0.2569, 0.0247, 0.0000, 0.0000, 0.1488, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0291, 0.0000, 0.0745, 0.1145])\n",
      "tensor([6.2471e+02, 4.1683e+01, 8.4654e+00, 2.5865e+01, 1.5301e+03, 1.4467e+04,\n",
      "        0.0000e+00, 5.2695e+06, 1.5901e+03, 2.4766e+00, 1.6878e+04, 6.9795e+01,\n",
      "        4.5215e+04, 2.0813e+04, 6.7299e+00, 2.8944e+04, 1.2916e+04, 1.2858e+01,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 8.6743e-01, 6.6137e+02, 2.1298e+05,\n",
      "        0.0000e+00, 2.9527e+04, 1.3855e+05, 5.0176e+06, 7.9262e+03, 4.6672e+06,\n",
      "        6.9638e+02, 1.1907e+06, 3.4997e+04, 2.4429e+04, 8.4756e+04, 7.0918e+01,\n",
      "        1.1126e+02, 3.1601e+03, 3.1523e+06, 2.7766e+02, 2.1181e+05, 5.2511e+06,\n",
      "        3.0189e+07, 7.4388e+03, 6.1183e+02, 4.1197e+00, 1.0857e+02, 1.5455e+04,\n",
      "        3.3786e+04, 3.9229e+02, 8.1891e+02, 0.0000e+00, 2.8583e+04, 1.5006e+02,\n",
      "        1.8069e+02, 0.0000e+00, 3.2413e+02, 2.6059e+05, 7.6941e+04, 2.5496e+02,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7018e+04,\n",
      "        2.4784e+05, 7.5902e+01, 0.0000e+00, 1.1995e+01, 3.9134e+03, 6.7821e+02,\n",
      "        2.6267e+01, 1.4555e+03, 6.7926e+04, 2.3494e+05, 3.6460e+05, 1.5353e+06,\n",
      "        3.0402e+01, 1.1319e+05, 1.4279e+07, 0.0000e+00, 3.4110e+07, 1.1918e+06,\n",
      "        1.4953e+06, 3.4857e+02, 3.5293e+05, 3.1960e+01, 1.5992e+06, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.5761e+02, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7849e+03, 9.2726e+04, 3.6809e+04,\n",
      "        2.3841e+01, 1.6345e+05, 9.1958e+04, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 3.9744e+02, 4.4217e+01, 2.6213e+04, 5.9765e+04,\n",
      "        2.4006e+02, 4.1193e+01, 2.7516e+04, 9.9535e+04, 3.4299e+05, 5.4406e+02,\n",
      "        1.1461e+03, 3.2862e+06, 2.4445e+03, 0.0000e+00, 0.0000e+00, 3.8403e+04,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 7.8023e+01, 1.9219e+01, 0.0000e+00, 4.7639e+03, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.5955e+01, 0.0000e+00, 8.7222e+04, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 3.9892e+02, 0.0000e+00, 0.0000e+00, 2.9446e+05,\n",
      "        0.0000e+00, 1.7190e+05, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        3.0101e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 2.1904e+05, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.7108e+02, 0.0000e+00, 1.9233e+04, 0.0000e+00,\n",
      "        1.2262e+01, 0.0000e+00, 1.2756e+02, 1.2359e+01, 2.1876e+06, 0.0000e+00,\n",
      "        1.1272e+05, 0.0000e+00, 0.0000e+00, 3.9435e+02, 9.8355e+05, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 8.5265e+02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 3.3470e+01, 1.9701e+04, 5.3175e+05, 0.0000e+00,\n",
      "        1.4681e+05, 0.0000e+00, 0.0000e+00, 6.5273e+05, 0.0000e+00, 1.0655e+05,\n",
      "        9.4738e+03, 0.0000e+00, 0.0000e+00, 2.0872e+04, 0.0000e+00, 4.0690e+05,\n",
      "        2.4935e+03, 2.5676e+01, 0.0000e+00, 0.0000e+00, 3.5418e+04, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        1.4592e+03, 0.0000e+00, 3.6687e+02, 1.7601e+00])\n"
     ]
    }
   ],
   "source": [
    "def integral_alpha_beta(f_t, tt, t0, beta, g):\n",
    "    f_i = lambda tx: f_t(tx,g)*torch.exp(beta[g]*tx.view(-1))\n",
    "    integration_domain = [[t0[g],tt]]\n",
    "    result = simp.integrate(f_i, dim=1, N=101, integration_domain=integration_domain)\n",
    "    return result\n",
    "\n",
    "val = []\n",
    "val2 = []\n",
    "for i in range(250):\n",
    "    val.append(integral_alpha_beta(tt=t[0,:][i],f_t=f_t,t0=t0,beta = beta,g = i))\n",
    "    val2.append(f_t(t=t[0,:][i],g = i))\n",
    "\n",
    "print(torch.tensor(val2))\n",
    "print(torch.tensor(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "28eb05f6-c7cb-4331-b43b-286d3fa29a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9999, 0.6188, 0.6188, 0.5963, 0.6424, 0.5937, 0.0000, 0.3205, 0.2336,\n",
      "         0.2794, 0.6613, 0.6513, 0.6587, 0.6592, 0.7878, 0.6757, 0.6487, 0.6536,\n",
      "         0.0000, 0.0000, 0.0000, 0.1511, 0.2522, 0.6611, 0.0000, 0.5082, 0.5250,\n",
      "         0.2819, 0.2478, 0.5365, 0.2397, 0.6621, 0.6570, 0.6544, 0.4560, 0.4619,\n",
      "         0.4570, 0.4690, 0.6590, 0.6602, 0.6613, 0.5605, 0.6292, 0.4057, 0.5273,\n",
      "         0.4721, 0.5381, 0.5297, 0.5883, 0.4780, 0.5593, 0.0000, 0.5928, 0.5800,\n",
      "         0.5940, 0.0000, 0.6476, 0.5144, 0.3598, 0.5014, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.6949, 0.6917, 0.4901, 0.0000, 0.5312, 0.5094, 0.5365,\n",
      "         0.5242, 0.6082, 0.6151, 0.5175, 0.6175, 0.3918, 0.6821, 0.5146, 0.5236,\n",
      "         0.0000, 0.6974, 0.8043, 0.6864, 0.6519, 0.6617, 0.7456, 0.6605, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.4037, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7406, 0.6624, 0.6619,\n",
      "         0.6624, 0.6589, 0.6614, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6497,\n",
      "         0.6554, 0.6658, 0.6531, 0.7904, 0.8681, 0.7929, 0.7903, 0.7863, 0.7869,\n",
      "         0.8385, 0.7880, 0.7884, 0.0000, 0.0000, 0.1086, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1193, 0.0409, 0.0000, 0.0620, 0.0000, 0.0000, 0.0000, 0.1123,\n",
      "         0.0000, 0.0131, 0.0000, 0.0000, 0.0000, 0.1695, 0.0000, 0.0000, 0.1208,\n",
      "         0.0000, 0.0218, 0.0000, 0.0000, 0.0000, 0.0000, 0.0099, 0.0000, 0.0000,\n",
      "         0.0000, 0.0862, 0.0000, 0.0000, 0.0000, 0.0378, 0.0000, 0.1168, 0.0000,\n",
      "         0.0363, 0.0000, 0.0990, 0.0299, 0.1375, 0.0000, 0.1346, 0.0000, 0.0000,\n",
      "         0.0612, 0.0819, 0.0000, 0.0000, 0.0000, 0.1950, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0868, 0.2406, 0.1009, 0.0000, 0.0757, 0.0000, 0.0000,\n",
      "         0.0389, 0.0000, 0.1266, 0.1182, 0.0000, 0.0000, 0.0776, 0.0000, 0.1700,\n",
      "         0.1601, 0.0247, 0.0000, 0.0000, 0.1488, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0254, 0.0000, 0.0737, 0.1167]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "0:00:00.030400\n"
     ]
    }
   ],
   "source": [
    "def integral_operator(tt,f_t, t0, beta, gamma):\n",
    "    t_scale = tt - t0 # when t0 is non-zero, minus t0 to make sure time start from 0\n",
    "    f_beta = lambda t: f_t(t,t_scale,t0)*t_scale\n",
    "    f_gamma = lambda t: f_t(t,t_scale,t0)*torch.exp(gamma*(t_scale*t+t0)*0)\n",
    "    integration_domain = [[0,1]]\n",
    "    int_beta = simp.integrate(f_beta, dim=1, N=101, integration_domain=integration_domain)\n",
    "    int_gamma = simp.integrate(f_gamma, dim=1, N=101, integration_domain=integration_domain)\n",
    "    return int_beta\n",
    "\n",
    "print(f_t2(1,t[0,:],t0))\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "integral_operator(tt=t[0,:],f_t=f_t2,t0=t0,beta = beta, gamma = gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "72ed7ac8-f1b0-48bc-9dd3-5cfce050f75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1988e+00, 1.3001e+03, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "g = 2\n",
    "t = t_real[10,:][2]\n",
    "h = torch.exp(f_t.log_h)[g,:].view(-1)\n",
    "phi = torch.exp(f_t.log_phi)[g,:].view(-1)\n",
    "tau = f_t.tau[g,:].view(-1)\n",
    "o = f_t.o[g,:].view(-1)\n",
    "w = f_t.grn[g,:].view(-1)\n",
    "bias = f_t.alpha_unconstr_bias[g]\n",
    "\n",
    "#emu = h[locate,:] * torch.exp(-phi[locate,:]*(T.reshape((dim,1))-self.tau[locate,:])**2) + self.o[locate,:]\n",
    "emu = h * torch.exp(-phi*(t - tau)**2) + o\n",
    "print(emu)\n",
    "## Use the Emulator matrix to predict alpha\n",
    "#emu = emu * self.grn[locate,:]\n",
    "#emu = emu * w\n",
    "\n",
    "#alpha_unconstr = emu.sum()\n",
    "#alpha_unconstr = alpha_unconstr + self.alpha_unconstr_bias[locate]\n",
    "#alpha_unconstr = alpha_unconstr + bias\n",
    "\n",
    "## Generate transcription kinetic rate for time t\n",
    "#alpha = torch.clamp(alpha_unconstr,0,)\n",
    "#alpha = F.softsign(alpha)\n",
    "#alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "9d7c5c15-e592-47d4-9d9b-07808af82e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 6.5331e+03, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7014660d-ce3d-46e1-baec-159acade0671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183, 2.7183, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "8e8a0030-9ed3-422e-b94a-ad20d45f4b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "80754d9e-0cba-40b9-a815-41f2080f9865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "762c566c-182c-4407-9026-26cfef61335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.1988e+00, 1.3001e+03, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37, 1.0186e-37,\n",
      "        1.0186e-37, 1.0186e-37, 1.0186e-37], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "t_scale = t_real[10,:]\n",
    "t = 1\n",
    "h = torch.exp(f_t.log_h)\n",
    "phi = torch.exp(f_t.log_phi)\n",
    "tau = f_t.tau\n",
    "o = f_t.o\n",
    "w = f_t.grn\n",
    "bias = f_t.alpha_unconstr_bias\n",
    "\n",
    "#emu = h[locate,:] * torch.exp(-phi[locate,:]*(T.reshape((dim,1))-self.tau[locate,:])**2) + self.o[locate,:]\n",
    "rt = t*t_scale+t0\n",
    "if len(rt.shape) == 1:\n",
    "    rt = rt.reshape(1,-1)\n",
    "\n",
    "### create new dimensions for parameters\n",
    "phi_rep = phi.repeat(rt.shape[0], 1, 1)\n",
    "phi_rep = phi_rep.view(rt.shape[0], phi.shape[0], phi.shape[1])\n",
    "\n",
    "o_rep = o.repeat(rt.shape[0], 1, 1)\n",
    "o_rep = o_rep.view(rt.shape[0], o.shape[0], o.shape[1])\n",
    "\n",
    "h_rep = h.repeat(rt.shape[0], 1, 1)\n",
    "h_rep = h_rep.view(rt.shape[0], h.shape[0], h.shape[1])\n",
    "\n",
    "tau_rep = tau.repeat(rt.shape[0], 1, 1)\n",
    "tau_rep = tau_rep.view(rt.shape[0], tau.shape[0], tau.shape[1])\n",
    "\n",
    "emu = h_rep[0,2,:] * torch.exp(-phi_rep[0,2,:]*(rt.unsqueeze(2)[0,2,0] - tau_rep[0,2,:])**2) + o_rep[0,2,:]\n",
    "print(emu)\n",
    "## Use the Emulator matrix to predict alpha\n",
    "#emu = emu * self.grn[locate,:]\n",
    "\n",
    "#w_rep = w.repeat(rt.shape[0], 1, 1)\n",
    "#w_rep = w_rep.view(rt.shape[0], w.shape[0], w.shape[1])\n",
    "#emu = emu * w_rep\n",
    "\n",
    "#alpha_unconstr = emu.sum(dim=2)\n",
    "#alpha_unconstr = alpha_unconstr + self.alpha_unconstr_bias[locate]\n",
    "#bias_rep = bias.repeat(rt.shape[0], 1)\n",
    "#alpha_unconstr = alpha_unconstr + bias_rep\n",
    "\n",
    "## Generate transcription kinetic rate for time t\n",
    "#alpha = torch.clamp(alpha_unconstr,0,)\n",
    "#alpha = F.softsign(alpha)\n",
    "#alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "5ecaab5e-b852-49d3-9e2e-4af0449d067c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 6.5331e+03, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "        1.0000e+00, 1.0000e+00, 1.0000e+00], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_rep[0,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "20c4f964-6021-49fa-b09a-ed4851aa5ffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.7183, 2.7183, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_rep[0,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "81b4bd55-ecb2-4a6e-9c09-6c806b7f43a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(9.2291)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rt.unsqueeze(2)[0,2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d22a690f-c871-496e-bad8-3594cc3f8b25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 10.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tau_rep[0,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "48a1c4c6-7ae6-4739-bd42-72d2bd1c9dda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_rep[0,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "54519a20-9411-4608-a35e-32cd6f580a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:03:10.756549\n"
     ]
    }
   ],
   "source": [
    "starttime = datetime.datetime.now()\n",
    "a,b = SolveInitialValueProblem(f_t = f_t, x0 = x0,t0 = t0, t_eval = t.T)\n",
    "endtime = datetime.datetime.now()\n",
    "print(endtime - starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "fa124018-ba79-4c4a-90c0-33b50189f788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 16.6076, 141.4018,  67.6779,   4.9476, 217.0428,  12.0619, 358.3701,\n",
       "           3.0464, 195.6053, 172.0990,   9.8396,   8.4261,   7.4889,  25.2139,\n",
       "          20.1298, 114.5480,  19.2105,   3.9829, 154.5477,  40.7347, 578.9570,\n",
       "         443.5404, 209.8475, 112.5899, 317.2336,  65.3547,   1.9354, 145.9795,\n",
       "         168.2146,  12.1935, 356.7754,   1.1908, 145.6228,   7.9605, 474.8371,\n",
       "           8.3131,  20.0553,  19.8439,  50.9583, 176.2853,  32.9334, 358.9092,\n",
       "         191.8283,   6.7982, 202.5353,  29.9600,  22.3159,   3.8278,  27.7432,\n",
       "          10.4419, 180.6682,   4.1057, 128.3981,   9.7866,  18.9930,  16.2818,\n",
       "           6.1908,  47.5848, 392.1898,  17.0861, 599.2476,  50.0054,  25.7103,\n",
       "         340.9219,   1.9223, 166.9864,   7.6977,  17.3980, 519.7316,  67.2864,\n",
       "           4.4295,   3.8262,  74.7777, 278.2805,  16.4839,  73.1085,  24.7631,\n",
       "          78.2489,   7.1848,  31.0928,  51.3320,  45.4606,   3.4379,  25.2979,\n",
       "          17.0837, 167.1650,  63.4664, 295.2383,  15.1994,  85.2300, 299.8301,\n",
       "           0.7714,  66.2332, 159.9934,   9.3643, 147.6670,  66.8262, 152.0979,\n",
       "         133.6596,  12.0242, 367.0347,  12.0886,  15.1061, 158.9929,  23.2675,\n",
       "          35.0463,  70.2094, 132.4872, 303.0565,  32.7299, 342.1054,  12.8499,\n",
       "          13.8581,   8.9863,  32.3703, 464.8356,   9.1614,   5.2999, 174.0928,\n",
       "         214.3075,   4.4104,  33.3577,   2.6537,  27.1895,  70.6137,  98.9158,\n",
       "           5.5029,  84.2191],\n",
       "        [  5.6470, 236.0993,   2.9687,   3.6787,  18.6782,  33.6490, 120.2446,\n",
       "          59.7625,   7.2434,  47.6418, 106.4091, 154.3337,  34.7790,  59.3375,\n",
       "          29.4489, 160.0605,  38.3118,  13.0768,  28.0844,   1.0955,  29.6229,\n",
       "          22.3233, 314.7935,   7.9941, 121.6754, 240.6706,  88.7853, 392.0462,\n",
       "         157.6038,   9.5304, 254.3084,   2.7979, 100.6624, 157.1978,  26.0763,\n",
       "         260.0807,   5.5081,   2.3173, 137.9307, 423.0558,  17.3071,   4.1993,\n",
       "          17.7082,  24.3719, 286.3218, 157.0353,   6.6401, 116.3106,  36.1695,\n",
       "         245.9275, 423.5421,  13.0580, 130.3283, 126.8941, 226.2798, 136.3712,\n",
       "         142.2493,   6.6770, 230.9868,  68.5130, 136.6813,   1.7863, 402.0532,\n",
       "           3.5720,  46.5876, 267.1470,  10.6320, 194.1412,   4.7105,  46.3547,\n",
       "          70.2402,  26.0524,  10.7880,   3.5059, 334.5667,   4.7421,  37.7957,\n",
       "          17.6381,   5.8586,   3.8501,  68.4765,  43.2834,  38.5703,  45.4953,\n",
       "          35.6675,  93.1269,   7.9056,  21.8782,  12.9762,  10.0978,  73.4171,\n",
       "         265.1650,  58.5028, 291.7381, 113.1851,   8.7204,  26.5803,  13.9120,\n",
       "           6.5425,   5.7568,   2.6151,  47.7113, 202.0678, 413.2416,   7.0179,\n",
       "         142.6931,  18.4728,   2.9676,  28.0065,   3.2813,   3.3223,  89.6628,\n",
       "          42.1452, 184.0202,  60.7478,   2.4189,  56.0623,   5.3182,  23.1080,\n",
       "          20.5783,   9.0280, 394.7428,   7.3390, 150.8956,  19.7212,  65.5268,\n",
       "          14.7138, 148.7806]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddf5fcb2-ec26-448e-85ab-585532dcd878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integral_alpha_beta(f_t, tt, t0, beta, g):\n",
    "    f_i = lambda t: f_t(t,g)*torch.exp(beta[g]*t)\n",
    "    integration_domain = [[t0[g],tt]]\n",
    "    result = simp.integrate(f_i, dim=1, N=101, integration_domain=integration_domain)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f0a4db2-1376-47ed-a91a-88ca965ccb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt2 = t[:,0]\n",
    "integral_tensor_alpha_beta = torch.tensor(list(map(lambda tt: integral_alpha_beta(tt=tt,f_t=f_t,t0=t0,beta = beta,g = 1), tt2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa0747c6-b0d1-464a-87d4-e7154fd9d8e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.8144e+00, 3.1680e+03, 7.1853e+00, 1.0543e+07, 6.1589e+03, 2.3187e+01,\n",
       "        1.5655e+03, 1.6132e+02, 8.6527e+04, 1.1143e+07, 9.9307e+03, 1.7259e+04,\n",
       "        1.7334e+06, 1.5068e+06, 7.7024e+01, 2.6948e+01, 3.8685e+00, 2.6380e+06,\n",
       "        1.5074e+04, 1.8439e+05, 1.9408e+03, 4.8340e+00, 6.9157e+02, 6.6871e+05,\n",
       "        6.8449e+01, 1.5221e+02, 6.0650e+05, 5.6958e+03, 2.8019e+03, 3.6626e+06,\n",
       "        9.1829e+05, 1.1096e+04, 1.4705e+07, 9.4920e+03, 2.8942e+01, 3.5842e+06,\n",
       "        6.8350e+01, 1.2146e+03, 5.6564e+07, 3.8825e+04, 1.9531e+04, 8.0093e+01,\n",
       "        4.5085e+02, 1.2060e+07, 5.0763e+03, 4.2380e+05, 8.6835e+02, 2.5860e+02,\n",
       "        7.4527e+02, 4.5351e+06, 5.4080e+03, 1.3778e+05, 3.4884e+05, 5.3770e+04,\n",
       "        4.2732e+05, 1.0718e+01, 4.5153e+02, 9.7291e+05, 6.6496e+02, 4.0661e+04,\n",
       "        2.9465e+03, 1.4752e+07, 2.8987e+05, 5.1078e+02, 2.8592e+04, 3.7677e+07,\n",
       "        2.0604e+02, 3.6338e+05, 1.1322e+01, 2.4162e+05, 6.7300e+03, 1.4739e+05,\n",
       "        5.9985e+01, 7.4261e+04, 4.8347e+05, 8.3915e+04, 5.5686e+07, 2.8774e+02,\n",
       "        3.5937e+03, 2.2826e+01, 1.0225e+05, 1.2517e+05, 1.4926e+02, 2.5923e+03,\n",
       "        2.0909e+02, 1.2904e+04, 7.5304e+01, 9.8410e+01, 1.5959e+02, 1.6753e+03,\n",
       "        3.9111e+04, 4.1034e+00, 3.9502e+05, 4.6672e+01, 1.6216e+01, 1.6615e+03,\n",
       "        2.4826e+04, 9.1152e+04, 2.2675e+03, 1.0490e+04, 3.2506e+07, 1.1848e+06,\n",
       "        1.4200e+07, 6.1022e+00, 1.7123e+06, 2.8268e+03, 5.6531e+04, 3.2627e+03,\n",
       "        1.6778e+05, 1.9231e+03, 3.2488e+05, 4.8802e+05, 5.1398e+06, 2.3780e+07,\n",
       "        1.9397e+04, 1.2711e+05, 7.5069e+03, 2.1643e+01, 1.9609e+02, 1.7928e+02,\n",
       "        6.2096e+04, 2.8149e+05, 1.1701e+02, 3.5387e+02, 4.1127e+01, 2.7991e+05,\n",
       "        3.2671e+07, 5.6427e+00])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "integral_tensor_alpha_beta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "20c3b0f6-91aa-401c-8779-a0a3e1da5ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.8184e+00, 3.1705e+03, 7.1902e+00,  ..., 2.7992e+05, 3.2672e+07,\n",
       "         5.6466e+00],\n",
       "        [2.3353e+07, 1.7111e+04, 1.2919e+04,  ..., 3.9792e+02, 1.5302e+05,\n",
       "         2.1643e+01],\n",
       "        [2.2204e+05, 7.0898e+06, 3.0740e+07,  ..., 7.5634e+03, 4.7903e+06,\n",
       "         2.3315e+01],\n",
       "        ...,\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00],\n",
       "        [3.3748e+02, 1.2055e+05, 5.8347e+05,  ..., 1.6824e+06, 7.3759e+06,\n",
       "         1.6380e+05],\n",
       "        [6.6162e+01, 1.8053e+06, 1.5742e+06,  ..., 2.7177e+02, 1.7108e+01,\n",
       "         1.3802e+05]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f23a17-e60a-4721-87c4-6f514319af22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
